{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":""},{"location":"android/activities/","title":"Activities","text":"<p>source</p>"},{"location":"android/activities/#concept","title":"Concept","text":"<ul> <li>Mobile apps don't have a central entry point like <code>main</code>. </li> <li>User journey in your app may begin in a non-deterministically way. Example: gmail app start from the launcher icon vs opening from share sheet.</li> <li>Android starts the activities of an app, instead of app as an atomic whole.</li> <li>Activity provides window in which app draws its UI.</li> </ul>"},{"location":"android/activities/#manifest","title":"Manifest","text":"<ol> <li>Activities must be declared in the manifest file:   <pre><code>&lt;manifest ...&gt;\n  &lt;application ...&gt;\n    &lt;activity android:name=\".FooActivity\" /&gt;\n</code></pre><ul> <li><code>android:name</code> is the only required attribute.</li> </ul> </li> <li> <p>To allow other apps to launch your activity through implicit intent, add an <code>intent-filter</code>.</p> called activitycaller activity <pre><code>&lt;activity android:name=\".FooActivity\"&gt;\n  &lt;intent-filter&gt;\n    &lt;action android:name=\"android.intent.action.SEND\" /&gt;\n    &lt;category android:name=\"android.intent.category.DEFAULT\" /&gt;\n    &lt;data android:mimeType=\"text/plain\" /&gt;\n</code></pre> <pre><code>startActivity(\n  Intent().apply {\n    action = Intent.ACTION_SEND\n    type = \"text/plain\"\n    putExtra(Intent.EXTRA_TEXT, \"here goes my text\")\n  }\n)\n</code></pre> <ul> <li><code>action</code> specifies that this activity sends data.</li> <li><code>category</code> as <code>DEFAULT</code> lets activity receive launch requests.</li> <li><code>data</code> specifies the type of data this activity can send.</li> </ul> </li> <li> <p>Caller and callee activities must share permission.</p> called activitycaller activity <pre><code>&lt;manifest&gt;\n  &lt;activity \n    android:name=\".FooActivity\"\n    android:permission=\"com.example.fooapp.permission.SHARE_POST\"&gt;\n</code></pre> <pre><code>&lt;manifest&gt;\n  &lt;uses-permission android:name=\"com.example.fooapp.permission.SHARE_POST\" /&gt;\n</code></pre> </li> </ol>"},{"location":"android/activities/#lifecycle","title":"Lifecycle","text":"<p>source</p> <p> </p> <ol> <li> <p><code>onCreate</code>: perform logic here that happens once in activity's life. e.g.:</p> <ul> <li>bind data to list</li> <li>associate activity with a <code>ViewModel</code></li> <li>instantiate class-scope variables.</li> <li>set layout of the activity.</li> </ul> <pre><code>onCreate -&gt; onStart\n</code></pre> </li> <li> <p><code>onStart</code>: it's now visible to the user. It used to be that activity would never remain in <code>STARTED</code> state for long. But on large screen devices, with multi/freeform window, that's no longer the case.</p> <pre><code>onStart -&gt; onResume\n</code></pre> </li> <li> <p><code>onResume</code>: now in focus and interactive.</p> <pre><code>onResume -&gt; onPause\n</code></pre> </li> <li> <p><code>onPause</code>: actitiy is no longer in focus. e.g. a dialog is partially obscuring the activity, or focus is on another activity in multi-window mode. This is very brief and doesn't have enough time to do network and/or disk calls (e.g. save state).</p> <pre><code>onPause -&gt; onResume / onStop\n</code></pre> </li> <li> <p><code>onStop</code>: Happens when another activity has taken over the full screen or before the activity is finished running and due for termination. Release resources, stop animations, save stuff to db (if can't find a better alternative) and any CPU-intensive work here as the main thread is no longer updating the UI.</p> <pre><code>onStop -&gt; onRestart / onDestroy\n</code></pre> </li> <li> <p><code>onDestroy</code>: Activity is either finishing for good or is being temporarily destroyed for configuration change. In the <code>ViewModel#onCleared()</code> of the activity, check <code>isFinishing()</code> to determine which of the two it is. </p> <pre><code>onDestroy -&gt; [onCreate]\n</code></pre> </li> </ol> <p>Examples of activity state changes:</p> <ol> <li>Configuration change: <code>onPause -&gt; onStop -&gt; onDestroy -&gt; onCreate -&gt; onStart -&gt; onResume</code></li> <li>Enter multi-window mode / resize: same as previously mentioned.</li> <li>Activity / disalog appears in foreground: <code>onPause &lt;-&gt; onResume</code> if partially covered. A further <code>onStop &lt;-&gt; onRestart -&gt; onStart</code> is fully covered.</li> <li>Home / Recents button button: same as if activity has been covered.</li> <li>Back button: <code>onPause -&gt; onStop -&gt; onDestroy</code>. However, if it's the root launcher activity:<ul> <li>on \\(\\le\\) Android 11: system finishes the activity, so same as above.</li> <li>on \\(\\ge\\) Android 12: moves activity to background, so same as home / recents button.</li> </ul> </li> <li>Killed by system: there is no guarantee that <code>onDestory</code> will be called.</li> </ol> <p>The system never kills an activity to free up memory, but instead the whole process.</p>"},{"location":"android/animation-transition/","title":"Navigation animations","text":"<p>source</p> <p>There are two mains ways to animation navigations:</p> <ol> <li><code>Animation</code> + <code>Animator</code></li> <li>Transitions framework</li> </ol>"},{"location":"android/animation-transition/#animation-animator","title":"Animation + Animator","text":"<p>In above example, the previous fragment <code>fades out</code> while the new fragment <code>slides in</code>.</p> res/anim/fade_out.xml<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;alpha xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:duration=\"@android:integer/config_shortAnimTime\"\n  android:interpolator=\"@android:anim/decelerate_interpolator\"\n  android:fromAlpha=\"1\"\n  android:toAlpha=\"0\" /&gt;\n</code></pre> res/anim/slide_in.xml<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;translate xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:duration=\"@android:integer/config_shortAnimTime\"\n  android:interpolator=\"@android:anim/decelerate_interpolator\"\n  android:fromXDelta=\"100%\"\n  android:toXDelta=\"0%\" /&gt;\n</code></pre> res/anim/fade_in.xml<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;alpha xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:duration=\"@android:integer/config_shortAnimTime\"\n  android:interpolator=\"@android:anim/decelerate_interpolator\"\n  android:fromAlpha=\"0\"\n  android:toAlpha=\"1\" /&gt;\n</code></pre> res/anim/slide_out.xml<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;translate xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:duration=\"@android:integer/config_shortAnimTime\"\n  android:interpolator=\"@android:anim/decelerate_interpolator\"\n  android:fromXDelta=\"0%\"\n  android:toXDelta=\"100%\" /&gt;\n</code></pre> <p>include these animations the part of transaction.</p> <pre><code>supportFragmentManager.commit {\n  setReorderingAllowed(true)\n  setCustomAnimations(\n    /* enter=    */ R.anim.slide_in,\n    /* exit=     */ R.anim.fade_out,\n    /* popEnter= */ R.anim.fade_in,\n    /* popExit   */ R.anim.slide_out\n  )\n  replace(R.id.fragment_container, fragment)\n  addToBackStak(null)\n}\n</code></pre> <p>Animations must be set before <code>replace</code>. Order matters.</p>"},{"location":"android/animation-transition/#transition-framework","title":"Transition framework","text":"<p>The alternative approach is to define within each fragment, their enter/exit transition. </p> res/transition/fade.xml<pre><code>&lt;fade xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:duration=\"@android:integer/config_shortAnimTime\"/&gt;\n</code></pre> res/transition/slide_right.xml<pre><code>&lt;slide xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:duration=\"@android:integer/config_shortAnimTime\"\n  android:slideEdge=\"right\" /&gt;\n</code></pre> outgoing fragment<pre><code>class FragmentA : Fragment() {\n  override fun onCreate(savedInstanceState: Bundle?) {\n    super.onCreate(savedInstanceState)\n    val inflater = TransitionInflater.from(requireContext())\n    exitTransition = inflater.inflateTransition(R.transition.fade)\n  }\n}\n</code></pre> incoming fragment<pre><code>class FragmentB : Fragment() {\n  override fun onCreate(savedInstanceState: Bundle?) {\n    super.onCreate(savedInstanceState)\n    val inflater = TransitionInflater.from(requireContext())\n    enterTransition = inflater.inflateTransition(R.transition.slide_right)\n  }\n}\n</code></pre>"},{"location":"android/animation-transition/#extras","title":"Extras:","text":"<p>Read more about postponing transitions and sharing views between enter-exiting fragments on source page.  </p>"},{"location":"android/fragments/","title":"Fragments","text":""},{"location":"android/fragments/#intro","title":"Intro","text":"<p>source</p> <ul> <li>Modular portion of the user interface within an activity.</li> <li>Defines and manages its own layout, has its own lifecycle, and can handle its own input events. </li> <li>Must be hosted by an activity or another fragment. </li> <li>The fragment\u2019s view hierarchy becomes part of, or attaches to, the host\u2019s view hierarchy.</li> <li>Unlike activities, </li> </ul>"},{"location":"android/fragments/#creating-fragment","title":"Creating fragment","text":"<pre><code>class ExampleFragment\n  : Fragment(R.layout.example_fragment)\n</code></pre> <pre><code>class ExampleFragment: Fragment() {\n\n  override fun onCreateView(\n    inflater: LayoutInflater, container: ViewGroup?,\n    savedInstanceState: Bundle?\n  ): View? {\n    return inflater.inflate(R.layout.example_fragment, container, false)\n  }\n}\n</code></pre>"},{"location":"android/fragments/#adding-to-an-activity","title":"Adding to an activity","text":"example_activity.xml<pre><code>&lt;androidx.fragment.app.FragmentContainerView\n  xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:id=\"@+id/fragment_container_view\"\n  android:layout_width=\"match_parent\"\n  android:layout_height=\"match_parent\"\n  android:name=\"com.example.ExampleFragment\" /&gt; \n</code></pre> <p>In declarative approach, <code>android:name</code> specifies the class name of the fragment to instantiate.</p> ExampleActivity.kt<pre><code>class ExampleActivity: AppCompatActivity(...) {\n\n  override fun onCreate(savedState: Bundle?) {\n    super.onCreate(savedState)\n\n    if (savedState != null) {\n      supportFragmentManager.commit {\n        setReorderingAllowed(true)\n        add&lt;ExampleFragment&gt;(R.id.fragment_container_view)\n      }\n    }\n  }\n}\n</code></pre> <p>In programatic approach, <code>android:name</code> is omitted. So no specific fragment is automatically instantiated.   Note that fragment is automatically restored from the <code>savedState</code>.</p>"},{"location":"android/fragments/#passing-arguments-to-fragment","title":"Passing arguments to fragment","text":"Pass from parent activity<pre><code>val bundle = bundleOf(\"foo\" to 42)\n\nsupportFragmentManager.commit {\n  setReorderingAllowed(true)\n  add&lt;ExampelFragment&gt;(\n    R.id.fragment_container_view,\n    args = bundle\n  )\n}\n</code></pre> Retrieve in fragment<pre><code>override onViewCreated(...) {\n  val answerToLife = requireArguments().getInt(\"foo\")\n}\n</code></pre>"},{"location":"android/fragments/#fragment-manager","title":"Fragment manager","text":"<p>source</p> <p>Manages the fragment back stack, such as adding or removing fragments in response to user interactions. It can be accessed from both activity and fragment like this:</p> <p></p> Example transaction<pre><code>supportFragmentManager.commit {\n  replace&lt;ExampleFragment&gt;(R.id.fragment_container)\n  setReorderingAllowed(true)\n  addToBackStack(\"name\") // Name can be null\n}\n</code></pre>"},{"location":"android/fragments/#setreorderingallowed-purpose","title":"<code>setReorderingAllowed</code> purpose","text":"<p>Ensures that any intermediaate fragments do not go through lifecycle changes and do not have their animations / transitions run. Intermediate fragments are one that are added and replaced right away.</p> <p>It should always be set. But it's not default for compatibility reasons.</p>"},{"location":"android/fragments/#addtobackstack-purpose","title":"<code>addToBackStack</code> purpose","text":"<p>Commits the transaction to back stack so user can later reverse it. Reversal is done transaction-by-transaction basis, and not just fragment-by-fragment.</p> <p><code>remove</code> w/o <code>addToBackStack</code>:  removed fragment is destroyed and user cannot navigate back to it.</p> <p><code>remove</code> with <code>addToBackStack</code>:  fragment is only <code>STOPPED</code> and later <code>RESUMED</code> when user navigates back. Its <code>view</code> is destroyed in this case.</p>"},{"location":"android/fragments/#setprimarynavigationfragment","title":"<code>setPrimaryNavigationFragment</code>","text":"<p>In split-view apps where app shows multiple sibling fragments on screen at same time, then one fragment must be designated the primary fragment to handle app's navigation.</p> <p>This fragment is first to receive <code>onBackPressed</code> and system will check if this fragment can handle it. </p> <p>This fragment is used by navigation component to resolve navigation actions.</p>"},{"location":"android/fragments/#class-vs-instance","title":"Class vs instance","text":"<p>source</p> <p>We can manually create an instance of a fragment and add it in a transaction:</p> <pre><code>val myFrag = ExampleFragment()\nadd(R.id.fragment_container, myFrag)\n</code></pre> <p>But recall from Adding to an activity that on configuration changes, fragment manager automatically recreates the fragment for us, no manually instantiated fragment for it to us.</p> <p>Internally it uses reflection to find and invoke a no-arg constructor of the fragment. That is, any custom constructor used the first time is not used during recreation. Create a fragment factory to solve this.</p> factory<pre><code>class MyFragmentsFactory(val dep: MyDependency)\n  : FragmentFactory() {\n\n  override fun instantiate(\n    classLoader: ClassLoader, \n    className: String\n  ): Fragment = when(loadFragmentClass(classLoader, className)) {\n    ExampleFragment::class.java -&gt; ExampleFragment(dep)\n    else -&gt; super.instantiate(classLoader, className)\n  }\n}\n</code></pre> In activity<pre><code>override fun onCreate(savedState: Bundle?) {\n  supportFragment.fragmentFactory = MyFragmentsFactory(dep)\n  super.onCreate(savedState)\n}\n</code></pre> <p>You create a factory that knows how to create one or more types of fragments, and then let the parent activity know to use this factory. With all this set up, update the fragment transaction to use class instead of an instance:</p> <pre><code>add&lt;ExampleFragment&gt;(R.id.fragment_container)\n</code></pre>"},{"location":"android/fragments/#fragment-transaction","title":"Fragment transaction","text":"<p>source</p> <p>Each set of fragment changes that we commit are called a transaction. </p> <ul> <li> <p><code>commit</code> is async. It schedules the transaction to run on main UI thread as soon as possible.</p> </li> <li> <p><code>commitNow</code> is synchronous, but it's incompatible with <code>addToBackStack</code>. </p> </li> <li> <p>We can use <code>commit</code> and then force immediate execution by <code>executePendingTransactions</code>.</p> </li> <li> <p>Order of actions within a commit is important. </p> </li> </ul>"},{"location":"android/fragments/#attaching-and-detaching-fragments","title":"Attaching and detaching fragments","text":"<p>source)</p> <p><code>detach</code> detaches a fragment from the UI, destroys its view hierarchy. However, fragment remains in <code>STOPPPED</code> state, same as being put in back stack and fragment manager continues to maintain its state. That is, it can be reused with <code>attach</code>.</p> <code>remove</code> <code>detach</code> view is destroyed but fragment itself is retained both view and instance are destroyed <code>onDestroyView onStop onPause</code> <code>+ onStop onDetach</code> <p>equivalently:</p> <code>add</code> <code>attach</code> for adding a new fragment not already part of the activity for reattaching a fragment that previously added then detached triggers full lifecycle <code>onAttach onCreate onCreateView onStart onResume</code> skips <code>onAttach onCreate</code> <p> Note that <code>onAttach/onDetach</code> are poorly named. </p>"},{"location":"android/fragments/#lifecycle","title":"Lifecycle","text":"<p>A fragment and its view have separate <code>Lifecycle</code>, managed independently. View's lifecycle can be accessed through <code>viewLifecycleOwner</code> in fragment.</p> <p></p> <ul> <li>Fragment when first instantiated is in <code>INITIALIZED</code> state. <code>findFragmentBy*</code> will not work yet.</li> <li>Not shown in diagram above, there are two more lifecyle events called <code>onAttach</code> and <code>onDetach</code>.</li> <li><code>onAttach</code> is called before even <code>onCreate</code> and its counterpart is called after <code>onDestroy</code>.</li> <li><code>onAttach</code> is called when it is added to a <code>FragmentManager</code> and its host. It is now active. <code>findFragmentBy*</code> starts working here.</li> <li><code>onDetach</code> is called when fragment is remove from <code>FragmentManager</code>. It is no longer active and cannot be found with <code>findFragmentBy*</code>.</li> </ul> Fragment View Notes <code>CREATED</code> added to <code>FragmentManager</code>. restore any saved state here. <code>CREATED</code> <code>INITIALIZED</code> <code>view</code> can now be retrieved. set up initiate state of the view in <code>onViewCreated</code> and start observing <code>LiveData</code>. <code>CREATED</code> <code>CREATED</code> any previous view state is restored. <code>STARTED</code> <code>STARTED</code> recommended to tie lifecycle-aware components to this state. safe to perform transactions on child fragment. <code>RESUMED</code> <code>RESUMED</code> Animations and transitions have finished. ready for interaction. ok to manually set focus. <code>CREATED</code> <code>CREATED</code> downwards, no longer visible. <code>CREATED</code> <code>DESTROYED</code> exist animations and transitions done. fragment's view has been detached from the window. <p><code>ON_STOP</code> event is the last point where it's safe to perform a fragment transaction.</p> <p></p>"},{"location":"android/fragments/#maximum-state","title":"Maximum state","text":"<p><code>setMaxLifecycle</code> can be included in transaction to cap the maximum lifecycle state of a fragment. Additionally, a fragment cannot have state greater than its parent.</p>"},{"location":"android/tasks/","title":"Tasks","text":"<p>source</p> <p>A collection of activities that user interact with when trying to do something in your app, arranged in a stack (i.e. back stack).</p> <ul> <li>Home screen is the starting place for most tasks. Tapping on an app icon creates a new task for that launcher activity if one is not already present.</li> <li>When currenct activity <code>A</code> starts another, <code>B</code>, then <code>B</code> is added to the task and pushed to the top of the back stack. <code>A</code> remains in stack but is stopped.     </li> <li>The <code>task</code> is a cohesive unit that can move the background as a whole when user starts another task or goes back to home screen.</li> <li>Activities in the start are never rearranged, and activities can only be added or removed from the top. So if <code>A</code> starts <code>B</code> and <code>B</code> starts <code>A</code>, system will not promote <code>A</code> to the top, instead it will be a new instance of <code>A</code> on top. </li> <li>Multi-window environment maintains separate tasks for each window.</li> </ul>"},{"location":"android/tasks/#manage-behaviour","title":"Manage behaviour","text":"<p>It is possible to interrupt the default behaviour of activities being added to the tasks, principally through manifest file and through the launch flags.</p>"},{"location":"android/tasks/#attribute-in-manifest","title":"Attribute in <code>manifest</code>","text":"<ul> <li><code>android:taskAffinity</code>: activities with same affinity belong in same task, same \"application\" from user's perspective. By default, all activities in an application have same affinity. Use this attribute to group them differently. <code>\"\"</code> is treated as no affinity, and instead it's inherited.</li> <li> <p><code>launchMode</code>: The values fall into two groups. First for normal launches, consider the task <code>[A B C D]</code> with a new intent arriving for <code>D</code> (which is at top):</p> Launch mode Multiple Instances? Result Notes <code>standard</code> Yes <code>[A B C D D]</code> New instance is created for every new intent <code>singleTop</code> Conditionally <code>[A B C D]</code> Not if there's already an instance at the top. Intent is routed to <code>onIntent</code> of existing instance at the top. <p>and for specialized launches:</p> Launch mode Multiple Instances? Notes <code>singleTask</code> Conditionally Create activity at the root of a new task or locate existing one in a with same affinity task. Back button still takes you to top of old task. <code>singleInstance</code> No <code>singleTask +</code> no other activity allowed in its task. <code>singleInstancePerTask</code> Conditionally Can only be at the root of the task, thus only one instance per task. <p>Let's say we have <code>[A B C]</code> in task and intent arrives for <code>A</code>. Both <code>singleTask</code> and <code>singleInstancePerTask</code> will finish <code>B</code> and <code>C</code>.</p> </li> </ul>"},{"location":"android/tasks/#launch-flags","title":"Launch flags","text":"<ul> <li><code>FLAG_ACTIVITY_SINGLE_TOP</code> same as <code>singleTop</code>.</li> <li><code>FLAG_ACTIVITY_NEW_TASK</code> same as <code>singleTask</code>.</li> <li><code>FLAG_ACTIVITY_CLEAR_TOP</code> finishes activities on top of its instance in back stack. Has no equivalent <code>launchMode</code>. Usually used with <code>singleInstance</code> and adds the aforementioned quirk to it.</li> </ul>"},{"location":"concurrency/intro/","title":"Concurrency","text":"<p>source</p>"},{"location":"concurrency/intro/#latency-vs-throughput","title":"Latency vs. Throughput","text":"<ul> <li>Latency is a measure of how long a single task takes from start to finish.</li> <li>Throughput measures the number of tasks a system can handle over a period of time.</li> </ul> <p>Think a bike doing multiple roundtrips to carry people between point <code>A</code> and point <code>B</code>, vs a bus doing the same. The bike has the lead in latency, but the bus will beat it with throughput advantage.</p>"},{"location":"concurrency/intro/#concurrency-vs-parallelism","title":"Concurrency vs. parallelism","text":"<p>\"Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.</p> <ul> <li>Parallelism is about multiple tasks running at the same time.</li> <li>Concurrency is multiple tasks that run in interleaving time slices. It's juggling.</li> </ul>"},{"location":"concurrency/intro/#amdahls-law","title":"Amdahl's law","text":"<p>\\[ \\text{speed up} = \\frac{1}{(1-P) + \\dfrac{P}{N}} \\]</p> <p>where \\(P\\) is the fraction of code that can be parallelized, and \\(N\\) is the number of workers (processor cores). The code that's parallelized is split amongst the \\(N\\) cores, whereas the sequential code can only be done by a single core.</p> <p> Latency speedup: a given task cannot finish faster than its slowest sequential part, no matter how many cores we throw at it. </p>"},{"location":"concurrency/intro/#gustafsons-law","title":"Gustafson's law","text":"<p>Speed up is not just latency, but also throughput. Amdahl's law assumes the work to be done as constant. But if we keep throwing larger and larger amount of work at our cores, the sequential parts will have less and less effect.</p> <p>\\[ \\text{speed up} = 1 + (N - 1) \\times P \\]</p> <p> Throughput speedup: we can do more tasks with more cores, even when individual tasks aren't finishing faster. </p>"},{"location":"graph/bfs/","title":"Breadth-first Search","text":"<p>BFS is a graph traversal and search algorithm. It explores the vertices of a graph in layers. </p> <ol> <li>First layer contains just the starting vertex \\(s\\).</li> <li>Second layer contains \\(s\\)'s neighbours.</li> <li>Third layer contains these neighbours' neighbours, and so on.</li> </ol> <p> layer-0layer-1layer-2layer-3</p>"},{"location":"graph/bfs/#pseudo-code","title":"Pseudo-code","text":"BFS(G, s)<pre><code>mark s visited\nQ.enqueue(s)\n\nwhile Q is not empty:\n  u = Q.dequeue()\n  print u\n\n  for (u, v) in G:\n    if v is not visited:\n      mark v visited\n      Q.enqueue(v)\n</code></pre>"},{"location":"graph/bfs/#concrete-implementation","title":"Concrete implementation","text":"KotlinUnit tests <pre><code>fun Graph.bfs(): HashMap&lt;Int, Int&gt; {\n  val predecessor = HashMap&lt;Int, Int&gt;()\n  for (s in vertices)\n    if (s !in predecessor)\n      bfs(s, predecessor)\n  return predecessor\n}\n\nfun Graph.bfs(\n  source: Int,\n  pred: HashMap&lt;Int, Int&gt; = HashMap(), // vertex-&gt;predecessor mapping\n): HashMap&lt;Int, Int&gt; {\n  pred[source] = source\n  val queue = ArrayList&lt;Int&gt;().apply { this.add(source) }\n\n  while (queue.isNotEmpty()) {\n    val u = queue.removeFirst()\n    print(\"${pred[u]}-$u \") // do anything here\n\n    for (v in neighbours(u)) {\n      if (v !in pred) {\n        pred[v] = u\n        queue.add(v)\n      }\n    }\n  }\n  return pred\n}\n</code></pre> <pre><code>import org.junit.jupiter.api.Assertions.assertEquals\nimport org.junit.jupiter.api.Test\n\nclass BreadthFirstSearchKtTest {\n\n  @Test\n  fun nullGraph() {\n    val graph = Graph()\n    graph.add(1, 2, 3, 4, 5)\n    assertEquals(\"{1=1, 2=2, 3=3, 4=4, 5=5}\", graph.bfs().toString())\n  }\n\n  @Test\n  fun triangle() {\n    val graph = Graph()\n    graph.connect(1, 2)\n    graph.connect(2, 3)\n    graph.connect(3, 1)\n    assertEquals(\"{1=1, 2=1, 3=1}\", graph.bfs().toString())\n  }\n\n  @Test\n  fun squareWithOneDiagonal() {\n    val graph = Graph()\n    graph.connect(1, 2)\n    graph.connect(1, 3)\n    graph.connect(2, 3)\n    graph.connect(2, 4)\n    graph.connect(3, 4)\n    graph.connect(3, 5)\n    graph.connect(4, 5)\n    assertEquals(\"{1=1, 2=1, 3=1, 4=2, 5=3}\", graph.bfs().toString())\n  }\n}\n</code></pre>"},{"location":"graph/bfs/#highlights","title":"Highlights","text":"<ul> <li>BFS is similar to iterative DFS but differs in two wasy:<ol> <li>It uses a queue instead of a stack.</li> <li>visited check is done before enqueuing the vertex and as opposed to delaying it to dequeuing time.</li> </ol> </li> <li>It finds the path with least hops (shortest path) in unweighted graphs.</li> <li>It has the running time of \\(O(V+E)\\).</li> </ul>"},{"location":"graph/dfs/","title":"Depth-first Search","text":""},{"location":"graph/dfs/#intro","title":"Intro","text":"<p>It's helpful to consider DFS in contrast to its sibling BFS. Where BFS is the cautious approach, clearing up the graph layer-by-layer, DFS is the head-first adventure in the maze.   BFS is akin to a group of people exploring a maze, splitting off into subgroups at each intersection. Whereas DFS is like a lone explorer uncovering a maze.</p> <p></p>"},{"location":"graph/dfs/#pseudo-code","title":"Pseudo-code","text":""},{"location":"graph/dfs/#basic-version","title":"Basic version","text":"<p> recursive DFS(G, s)<pre><code>mark s visited\nprint u\nfor (s, u) in G:\n  if u is not visited:\n    DFS(G, u)\n</code></pre> iterative DFS(G, s)<pre><code>S.push(s)\nwhile S is not empty:\n  u = S.pop()\n  if u is not visited:\n    mark u visited\n    print u\n    for (u, v) in G:\n      S.push(v)\n</code></pre> <p>Recursive version is DFS at its simplest. Notice that it's not a one-to-one conversion to the iterative version.</p> <p>Iterative DFS is quite like BFS, except:  a) stack instead of queue.  b) vertex is checked for being visited after taking them out of the bag, instead of before being \"bagged\". </p>"},{"location":"graph/dfs/#explanation","title":"Explanation","text":"<p>If we just replace queue with stack in BFS, that'd not be enough for it to be DFS. <code>DFS(G, 2)</code> would spit out <code>2, 3, 1, 4</code>. The strategy of \"carry on\" exploring a path gets disrupted when the next vertex on said path is already marked visited. </p> <p> 1234xDid not \"push onwards\"</p> <p>The right approach then is to postpone marking vertices visited. This gives us the right result <code>2, 3, 4, 1</code>. Let vertices be pushed to the stack multiple times; visit them from the most recently traversed edge.</p> <p> 1234</p> <p>i.e. Give multiple edges: \\(a \\rightarrow t, b \\rightarrow t, c \\rightarrow t\\) leading to the same destination vertex \\(t\\); the incomplete version of DFS (i.e. just BFS with stack) takes the first encountered edge \\(a \\rightarrow t\\) to \\(t\\). The complete solution is to pick the last encountered edge \\(c \\rightarrow t\\).</p>"},{"location":"graph/dfs/#dfs-tree","title":"DFS tree","text":"<p>If we are to trace the above DFS algorithm running on a graph, it'd lead us to a tree structure. Let's call it \\(G_{\\pi}\\). \\(G_{\\pi}\\) will have all the vertices of \\(G\\), meaning it'll be a spanning tree (or forest if there are multiple connected components). However, not all the edges of \\(G\\) will be present in \\(G_{\\pi}\\). We classify edges of \\(G\\) in following manner:</p> <ol> <li>Tree edge: an edge of \\(G\\) that is also present in \\(G_{\\pi}\\). This is an edge DFS actually traversed.</li> <li>Back edge: an edge of \\(G\\) leading a vertex back to its ancestor (proper or parent) in \\(G_{\\pi}\\).</li> <li>Forward edge: an edge of \\(G\\) leading a vertex to its proper descendant in \\(G_{\\pi}\\). </li> <li>Cross edge: all other edges.</li> </ol> <p>An undirected graph can only have #1 and #2, as there are no one-way edges. DFS will traverse would-be forward/cross edges, turning them into tree edges or back edges. </p> <p>From this, identifying a back edge in an undirected graph is straightforward:</p> recursive DFS(G, s)<pre><code>mark s visited\nprint u\nfor (s, u) in G:\n  if u is not visited:\n    DFS(G, u)\n  else:\n    error(\"back edge. loop detected.\")\n</code></pre> iterative DFS(G, s)<pre><code>S.push(s)\nwhile S is not empty:\n  u = S.pop()\n  if u is not visited:\n    mark u visited\n    print u\n    for (u, v) in G:\n      S.push(v)\n  else:\n    error(\"back edge. loop detected.\")\n</code></pre>"},{"location":"graph/dfs/#vertex-coloring","title":"Vertex coloring","text":"<p>Introduce a third state after visited, called explored to mark vertices whose neighbours are all visited. When an edge \\(u \\rightarrow v\\) is first traversed, the color of \\(v\\) determines the type of this edge:</p> <ol> <li><code>WHITE</code> indicates it's a tree edge.</li> <li><code>GRAY</code> indicates it's a back edge.</li> <li><code>BLACK</code> indicates it's a forward or cross edge.</li> </ol> recursivekotliniterativekotlin <pre><code>DFS(G):\n  for u in G:\n    u.color = WHITE\n  for u in G:\n    if u.color is WHITE:\n      DFS(G, u)\n\nDFS(G, u):\n  u.color = GRAY\n  for (u, v) in G:\n    switch(v.color):\n      WHITE: DFS(G, v)\n      GRAY:  # u -&gt; v is back edge\n      BLACK: # u -&gt; v is forward/cross edge\n  u.color = BLACK\n</code></pre> <pre><code>fun Graph.dfs() {\n  val color = HashMap&lt;Int, Color&gt;()\n  for (u in vertices)\n    if ((color[u] ?: Color.WHITE) == Color.WHITE)\n      dfs(u, u, 0, color)\n}\n\nfun Graph.dfs(\n  source: Int,\n  parent: Int = source,\n  indent: Int = 0,\n  color: HashMap&lt;Int, Color&gt; = HashMap()\n) {\n  println(\" \".repeat(indent) + \"$parent-$source tree\")\n  color[source] = Color.GRAY\n\n  val ci = indent + \"$parent-\".length\n  for (n in neighbours(source)) {\n    when(color[n] ?: Color.WHITE) {\n      Color.WHITE -&gt; dfs(n, source, ci, color)\n      Color.GRAY -&gt; println(\" \".repeat(ci) + \"$source-$n back\")\n      Color.BLACK -&gt; println(\" \".repeat(ci) + \"$source-$n forward/cross\")\n    }\n  }\n  color[source] = Color.BLACK\n}\n\nenum class Color { WHITE, GRAY, BLACK }\n</code></pre> <pre><code>DFS(G):\n  for u in G:\n    u.color = WHITE\n  for u in G:\n    if u.color is WHITE:\n      DFS(G, u)\n\nDFS(G, s):\n  S.push({\u2205, s, false})\n  while S is not empty:\n    p, u, explored = S.pop()\n    if explored:\n      u.color = BLACK\n      continue\n\n    print p -&gt; u\n\n    switch(u.color):\n      WHITE:\n        u.color = GRAY\n        S.push({p, u, true}) # to later mark u as explored\n        for (u, v) in G:\n          S.push({u, v})\n      GRAY:  # p -&gt; u is back edge\n      BLACK: # p -&gt; u is forward/cross edge\n</code></pre> <pre><code>fun Graph.dfs() {\n  val color = HashMap&lt;Int, Color&gt;()\n  for (u in vertices)\n    if (color.getOrDefault(u, Color.WHITE) == Color.WHITE)\n      dfs(u, color)\n}\n\nfun Graph.dfs(\n  source: Int,\n  color: HashMap&lt;Int, Color&gt; = HashMap(),\n) {\n  val stack = ArrayDeque&lt;Record&gt;().apply { this.add(Record(source, source)) }\n  while (stack.isNotEmpty()) {\n    val r = stack.removeFirst()\n    val c = color.getOrDefault(r.src, Color.WHITE)\n    if (r.explored) {\n      color[r.src] = Color.BLACK\n      continue\n    }\n    println(\"$r ${c.type}\")\n\n    if (c == Color.WHITE) {\n      color[r.src] = Color.GRAY\n\n      // set up a memo to mark u `BLACK` when all neighbours are explored.\n      stack.addFirst(r.copy(explored = true))\n\n      // push neighbours in reverse order to mimic recusive dfs output.\n      for (v in neighbours(r.src).reversed())\n        stack.addFirst(Record(r.src, v, r.childIndent()))\n    }\n  }\n}\n\ndata class Record(\n  val pred: Int, \n  val src: Int, \n  val indent: Int = 0, \n  val explored: Boolean = false\n) {\n  fun childIndent(): Int = indent + \"$pred-\".length\n  override fun toString(): String {\n    if (explored) return \"$src explored\"\n    return \" \".repeat(indent) + \"$pred-$src\"\n  }\n}\n\nenum class Color(val type: String) {\n  WHITE(\"tree\"), GRAY(\"back\"), BLACK(\"forward/cross\")\n}\n</code></pre> <ol> <li>hello</li> </ol>"},{"location":"graph/dfs/#example-run","title":"Example run","text":"<p> <pre><code>1-1 tree\n  1-2 tree\n    2-1 back\n    2-3 tree\n      3-2 back\n      3-1 back\n      3-4 tree\n        4-2 back\n        4-1 back\n        4-3 back\n    2-4 forward/cross\n  1-3 forward/cross\n  1-4 forward/cross\n</code></pre> <p> 1234</p> <pre><code>0-0 tree\n  0-2 tree\n    2-0 back\n    2-6 tree\n      6-2 back\n      6-4 tree\n        4-6 back\n        4-5 tree\n          5-4 back\n          5-0 back\n          5-3 tree\n            3-4 back\n            3-5 back\n        4-3 forward/cross\n        4-7 tree\n          7-4 back\n          7-1 tree\n            1-7 back\n          7-0 back\n  0-5 forward/cross\n  0-7 forward/cross\n</code></pre> <p> 02543716</p> <pre><code>1-1 tree\n  1-2 tree\n3-3 tree\n4-4 tree\n  4-3 forward/cross\n5-5 tree\n  5-9 tree\n    9-14 tree\n      14-15 tree\n         15-11 tree\n            11-8 tree\n               8-3 forward/cross\n               8-4 forward/cross\n            11-12 tree\n               12-8 forward/cross\n               12-16 tree\n         15-12 forward/cross\n6-6 tree\n  6-2 forward/cross\n  6-7 tree\n    7-3 forward/cross\n    7-11 forward/cross\n  6-12 forward/cross\n10-10 tree\n   10-13 tree\n      13-9 forward/cross\n   10-14 forward/cross\n   10-11 forward/cross\n</code></pre> <p> 12345678910111213141516</p>"},{"location":"graph/dfs/#parenthesis-property","title":"Parenthesis property","text":"<p>Consider any two vertices \\(u\\) and \\(v\\): the ranges \\([u_{s}, u_{f}]\\) and \\([v_{s}, v_{f}]\\) will either contain each other or will be completely disjoint, never partially overlapping. An edge \\(u \\rightarrow v\\) is then: </p> <ol> <li>Tree/forward edge if \\( \\underset{u}{\\big[} \\; \\underset{v}{\\big[} \\; \\underset{v}{\\big]} \\; \\underset{u}{\\big]} \\).</li> <li>Back edge if \\( \\underset{v}{\\big[} \\; \\underset{u}{\\big[} \\; \\underset{u}{\\big]} \\; \\underset{v}{\\big]} \\).</li> <li>Cross edge if \\( \\underset{u}{\\big[} \\; \\underset{u}{\\big]} \\; \\underset{v}{\\big[} \\; \\underset{v}{\\big]} \\) or \\( \\underset{v}{\\big[} \\; \\underset{v}{\\big]} \\; \\underset{u}{\\big[} \\; \\underset{u}{\\big]} \\).</li> </ol>"},{"location":"graph/generic-search/","title":"Generic graph search","text":""},{"location":"graph/generic-search/#about","title":"About","text":"<p>The algorithms we explore later are all specialiazation of this generic search algorithm.</p>"},{"location":"graph/generic-search/#pseudo-code","title":"Pseudo-code","text":"GenericSearch(G, s)<pre><code>bag.add(s)\nwhile bag is not empty:\n  u = bag.remove()\n  if u is not visited:\n    mark u visited\n    for (u, v) in G:\n      bag.add(v)\n</code></pre> <p>where bag is a placeholder ADT.</p>"},{"location":"graph/generic-search/#variants","title":"Variants","text":"<ol> <li>Stack: using a stack as \"bag\" gives us DFS. This spits out a depth-first spanning tree, which in general is going to be thin and deep.</li> <li>Queue: this gives us BFS. The spanning tree from this will be wide and shallow.</li> <li>Priority queue: this is basically a family of algorithm on its own.<ul> <li>Edge weights as priority gives an algorithm for discovering Minimum spanning tree, roughly Prim's algorithm.</li> <li>Vertex distance as priority gives shortest-path algorithm in weighted graphs, roughly Dijkstra's algorithm.</li> </ul> </li> </ol>"},{"location":"graph/intro/","title":"Introduction","text":"<p>Formally, a graph \\(G\\) is a set of vertices \\(V\\) and a set of edges \\(E\\) that connect said vertices. We'd be limiting ourselves to simple graphs which disallow duplicate (parallel) edges and edges connecting a vertex to itself (self loop).</p> <p>In an undirected graph, edges are denoted as unordered pair \\(\\{u, v\\}\\) while in a directed graph, they are ordered pair \\((u, v)\\).</p>"},{"location":"graph/intro/#glossary","title":"Glossary","text":"<p>Null/empty graph: there are no edges between the vertices.</p> <p> </p> <p>Connected graph: where there is a path between every given pair of vertices. If graph is undirected then \\(V-1 \\le E \\le {V\\choose2}\\).</p> <p> </p> <p>Complete graph: where an edge exists between every given pair of vertices. There will be exactly \\({V\\choose2} = \\frac{V \\cdot (V-1)}{2}\\) edges  in an undirected complete graph.</p> <p> </p> <p>Planar graph: which can be drawn on a plane such that its edges don't intersect.</p> <p> non-planarplanar</p> <p>Bipartite graph: where vertices can be divided in two sets such that no edge exists between the vertices of same set.</p> <p> </p> <p>Directed acyclic graph: is a directed graph with no cycles.</p> <p> </p> <p>Connected component: is set of vertices in a graph which are reachable from each other. A connected graph thus has a single connected component.</p> <p> </p> <p>Strongly connected component: is the same thing as connected components, but in directed graphs.</p> <p> </p> <p>Condensation graph: if each strongly connected component of a graph \\(G\\) is contracted to a single vertex, the resulting graph is called the condensation of \\(G^{SCC}\\), and is a DAG.</p> <p> Gcondensation of G</p> <p>Hamiltonian path: is a path that visits each vertex exactly once. A Hamiltonian cycle/circuit is the same but is a cycle. The problem is NP-complete.</p> <p> </p> <p>Euler walk: same thing but with each edge being used once.</p> <p> 11058749623</p>"},{"location":"graph/intro/#implementation","title":"Implementation","text":"PlainWeighted <pre><code>class Graph {\n  // Use a [HashSet] instead to disallow parallel edges out of the box.\n  private val adjacency: MutableMap&lt;Int, MutableList&lt;Int&gt;&gt; = HashMap()\n\n  val vertices: List&lt;Int&gt;\n    get() = adjacency.keys.stream().toList()\n\n  fun neighbours(vertex: Int): List&lt;Int&gt; = adjacency[vertex] ?: listOf()\n\n  fun add(vararg vertices: Int) {\n    for (v in vertices)\n        adjacency.putIfAbsent(v, ArrayList())\n  }\n\n  fun connect(src: Int, dst: Int) {\n    add(src, dst)\n    if (src == dst) return   // disallow self-loops.\n\n    adjacency[src]?.add(dst)\n    adjacency[dst]?.add(src) // Remove this for directed graph.\n  }\n}\n\n// (Optional) Use a dedicated class to represent vertices.\ndata class Vertex(val label: String) {\n  constructor(label: Int) : this(label.toString())\n  override fun toString() = label\n}\n</code></pre> <pre><code>class Graph {\n  private val adjacency: MutableMap&lt;Int, MutableList&lt;Edge&gt;&gt; = HashMap()\n  ...\n}\n\ndata class Edge(val dst: Int, val weight: Int)\n</code></pre>"},{"location":"graph/topological-sort/","title":"Topological Sort","text":""},{"location":"graph/topological-sort/#about","title":"About","text":"<p>Topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every edge \\((u, v)\\), \\(u\\) comes before \\(v\\) in the ordering.</p> <p>Such an ordering is only possible if there are no cycles in the graph, i.e. it's a DAG.</p>"},{"location":"graph/topological-sort/#example","title":"Example","text":"Valid topological sortings<pre><code>1 2 3 4 5\n1 2 3 5 4\n</code></pre> <p> 12345</p> Valid topological sortings<pre><code>5 6 1 4 2 3\n6 5 4 2 3 1\n</code></pre> <p> 156342</p> Valid topological sorting<pre><code>10 13 6 7 5 9 14 15 11 12 16 8 4 3 1 2\n1 5 6 10 2 7 13 9 14 15 11 12 8 16 4 3\n</code></pre> <p> 12345678910111213141516</p>"},{"location":"graph/topological-sort/#dfs-based-algorithm","title":"DFS based algorithm","text":"<p>DFS will finish processing a vertex \\(u\\) before its predecessor \\(p\\), i.e. \\(p_{post} \\gt u_{post}\\). That is, root vertices will have the highest <code>post</code> and leaf vertices will have the lowest <code>post</code>. So laying out vertices in the descending order of \\(\\text{post}\\) time will give us their topological ordering. Additionally, if we keep track of each vertex's color, we can also detect cycles (i.e. edge to a <code>GRAY</code> neighbour) and know when topological ordering is not possible.</p> pseudocodekotlin <pre><code>topologicalSort(G):\n  out = []\n  for u in G:\n    if u is not visited:\n      DFS(G, u)\n  return out\n\nDFS(G, u):\n  u.color = GRAY\n\n  for each edge (u, v):\n    if v.color is WHITE:\n      DFS(G, v)\n    else if v.color is GRAY:\n      error \"graph has cycle(s)\"\n\n  u.color = BLACK\n  out = [u, ...out] # prepend\n</code></pre> <pre><code>fun Graph.topologicalSort(): List&lt;Int&gt;? {\n  val result = ArrayList&lt;Int&gt;()\n  val color = HashMap&lt;Int, Color&gt;()\n  for (u in vertices)\n    if (color.getOrDefault(u, Color.WHITE) == Color.WHITE)\n      if (!topologicalSort(u, color, result))\n        return null\n  return result\n}\n\nfun Graph.topologicalSort(\n  source: Int,\n  color: HashMap&lt;Int, Color&gt;,\n  result: ArrayList&lt;Int&gt;,\n): Boolean {\n  val stack = ArrayDeque&lt;Record&gt;().apply { this.add(Record(source)) }\n  while (stack.isNotEmpty()) {\n    val (u, explored) = stack.removeFirst()\n    if (explored) {\n      color[u] = Color.BLACK\n      result.addFirst(u)\n      continue\n    }\n\n    when(color[u]) {\n      null, Color.WHITE -&gt; {\n        color[u] = Color.GRAY\n        stack.addFirst(Record(u, true))\n        for (v in neighbours(u))\n          stack.addFirst(Record(v))\n      }\n      Color.GRAY -&gt; return false\n      else -&gt; {}\n    }\n  }\n  return true\n}\n\nenum class Color { WHITE, GRAY, BLACK }\n\ndata class Record(val vertex: Int, val explored: Boolean = false)\n</code></pre>"},{"location":"graph/topological-sort/#kahns-algorithm","title":"Kahn's algorithm","text":"<p>The general idea behind this solution is that topological ordering will place source vertices before sink vertices. So we begin by listing all source vertices first (i.e. \\(\\text{in-degree} = 0\\)), followed by their neighbours, followed by their neighbour's neighbours and so on. </p> pseudocodekotlin <pre><code>result = []\nqueue = list of all source vertices # in-degree = 0\n\nwhile queue is not empty:\n  u = queue.any() # order doesn't matter\n  result.add(u)\n\n  # Derive the new set of \"source\" vertices\n  for (u, v) in G:\n    remove (u, v) from G\n    if --v.indegree is 0:\n      queue.add(v)\n\nif G has edges:\n  error \"graph has cycle(s)\"\nreturn result\n</code></pre> <pre><code>fun Graph.topologicalSort(): List&lt;Int&gt;? {\n  val degree = HashMap&lt;Int, Int&gt;()\n  for (u in vertices) {\n    degree.putIfAbsent(u, 0)\n    for (v in neighbours(u))\n      degree[v] = (degree[v] ?: 0) + 1\n  }\n\n  val result = ArrayList&lt;Int&gt;()\n  val queue = degree.filter { it.value == 0 }.map { it.key }.toCollection(ArrayList())\n  while (queue.isNotEmpty()) {\n    val u = queue.removeFirst()\n    result.add(u)\n\n    for (v in neighbours(u)) {\n      degree[v] = (degree[v] ?: 0) - 1\n      if (degree[v] == 0) queue.add(v)\n    }\n  }\n\n  if (degree.map { it.value }.sum() &gt; 0) return null\n  return result\n}\n</code></pre>"},{"location":"heap/","title":"Heap","text":""},{"location":"heap/#priority-queue","title":"Priority Queue","text":"<p>Before we talk about heaps, lets first talk about Priority queue. Priority queue is an abstract data type (ADT) that's a generalization of regular queue and stack ADT.</p> <p>Each element in a \\(PQ\\) has an associated priority. Elements with high priority are served before elements with lower priority. With this, we can think of a queue and stacks as specialization of \\(PQ\\):</p> <ul> <li>Queue: priority \\(\\propto \\text{time in queue}\\)</li> <li>Stack: priority \\(\\propto \\frac{1}{\\text{time in stack}}\\)</li> </ul>"},{"location":"heap/#heap_1","title":"Heap","text":"<p>Heap is a complete binary tree, i.e. a tree in which every level, except possibly the last, is completely filled. And all nodes in the last level are as far left as possible.</p> <p>A heap then also satisfies the heap property:</p> <ol> <li>if a max heap, any given node \\(n\\) is \\(\\ge\\) its children. The largest value is then at the root of the tree.</li> <li>if a min heap, any given node \\(n\\) is \\(\\le\\) its children. The smallest value is then at the root of the tree.</li> </ol>"},{"location":"heap/#index-relation","title":"Index relation","text":"<p>Heaps are usually implemented in arrays such that:</p> <ol> <li>every node has atmost \\(d = 2\\) children.</li> <li>a node at \\(i^{th}\\) index:<ul> <li>has its left child at index \\(2i + 1\\) and right child at \\(2i + 2\\).</li> <li>or in generalized term: \\(\\in [di+1, di + d]\\). </li> </ul> </li> <li>a node at \\(i^{th}\\) index:<ul> <li>has its parent at index \\(\\frac{i-1}{d}\\)</li> </ul> </li> </ol>"},{"location":"heap/#implementation","title":"Implementation","text":"sinkkotlinswimkotlinheapifykotlin <pre><code>sink(nums, i):\n  while i &lt; nums.size:\n    l = 2i + 1\n    r = 2i + 2\n\n    minIndex = indexOf(min(nums[i], nums[l], nums[r]))\n    if minIndex == i:\n      return\n\n    swap(i, minIndex)\n    i = minIndex\n</code></pre> <pre><code>fun ArrayList&lt;Int&gt;.sink(index: Int = 0, comparator: Comparator&lt;Int&gt; = naturalOrder()) {\n  var i = index\n  while (i &lt; size) {\n    val l = 2 * i + 1\n    val r = 2 * i + 2\n    var max = i\n\n    if (l &lt; size &amp;&amp; comparator.compare(this[l], this[max]) &lt; 0) max = l\n    if (r &lt; size &amp;&amp; comparator.compare(this[r], this[max]) &lt; 0) max = r\n\n    if (max == i) return\n\n    swap(max, i)\n    i = max\n  }\n}\n</code></pre> <pre><code>swim(nums, i):\n  while i &gt; 0:\n    p = (i - 1) / 2\n\n    minIndex = indexOf(min(nums[i], nums[p]))\n    if minIndex == p:\n      return\n\n    swap(i, p)\n    i = p\n</code></pre> <pre><code>fun ArrayList&lt;Int&gt;.swim(index: Int = size - 1, comparator: Comparator&lt;Int&gt; = naturalOrder()) {\n  var i = index\n  while (i &gt; 0) {\n    val p = (i - 1) / 2\n    if (comparator.compare(this[p], this[i]) &lt; 0) return\n\n    swap(i, p)\n    i = p\n  }\n}\n</code></pre> <pre><code>heapify(nums):\n  for i in [size / 2, 0]:\n    sink(i)\n</code></pre> <pre><code>fun ArrayList&lt;Int&gt;.heapify(comparator: Comparator&lt;Int&gt; = naturalOrder()) {\n  for (i in size / 2 downTo 0)\n    sink(i, comparator)\n}\n</code></pre> <p>Both <code>sink</code> and <code>swim</code> have running time of \\(O(log_d(n))\\), since that's the height of the tree. <code>heapify</code> gets ammortized to \\(O(n)\\).</p>"},{"location":"heap/#optimal-arity","title":"Optimal arity","text":"<p>Performance-wise \\(2-way \\lt 3-way \\approx 4-way \\gt 5-way\\). Really it depends on the ratio of insert vs deletes, but generally one would get best performance at \\(3\\) or \\(4\\).</p>"},{"location":"js/object-basics/","title":"Basics","text":"<p>An object is a composite value: an unordered collection of properties, each with a name and a value. Property names are typically strings, but can also be symbols.</p> <p>Object creation comes in two forms: literal and constructed form.</p>"},{"location":"js/object-basics/#object-literal","title":"Object literal","text":"<pre><code>const user = {\n  name: \"Foo\",\n  age: 42\n};\n\nuser.name; // 'Foo'\nuser.age; // 42\n\n// leaves user = { name: 'Foo' }\ndelete user.age; // true\n\nuser['name']; // 'Foo'\n</code></pre> <p>keys can be computed as well:</p> <pre><code>let key = \"umm\";\n\nconst user = {\n  \"foo bar\": 123,\n  [key + ' hey']: 456\n};\n</code></pre> <p>If property name and value are the same, we can use the shorthand notation:</p> <pre><code>const user = {\n  name,  // name: name\n  age,   // age : age\n  patron: 'zeus'\n}\n</code></pre>"},{"location":"js/object-basics/#object-constructor","title":"Object constructor","text":"<p><code>{}</code> syntax is good enough for one-off objects, but how do we create blueprint for a set of objects, all belonging to same \"class\"?</p> <p>For this we use constructor calls. When a function in JS is invoked with <code>new</code> operator, JS hijacks its behaviour so:</p> <pre><code>function User(name) {\n  // this = {};       # implicit\n  this.name = name;\n  greet() {\n    console.log(`hi ${this.name}`);\n  }\n  // return this;     # implicit\n}\n\nconst user = new User(\"foo\"); // { name: 'foo' }\n</code></pre> What about return statement <p>If the function being called has a return statement, it's ignored for primitive return, but not for object.</p> <pre><code>function User(name){ \n  this.name = name; \n  return 5; \n}\n\nnew User('foo'); // User {name: 'foo'}\n</code></pre> <pre><code>function User(name){ \n  this.name = name; \n  return { oink: 'oink' }; \n}\n\nnew User('foo'); // { oink: 'oink }\n</code></pre> <p>There are built-in constructor for object, arrays, dates, and boxed primtives:</p> <pre><code>const o = new Object(); // same as {}.\nconst a = new Array();  // same as [].\n</code></pre>"},{"location":"js/object-basics/#prototypes","title":"Prototypes","text":"<p>Constructor calls do more than just help out with boilerplate. They spit out objects which all point to same prototype. What is that?</p> JS vs. classic OOP <p>In every other OOP language one works with classes which are object blueprints. Objects are instances of classes. A class <code>C</code> which subclasses another <code>P</code> inherits its methods.</p> <p>JS doesn't work this way. For one, there was* no construct for classes, just objects. Secondly, inheritance worked via delegation. Object <code>C</code> would inherit functionality of parent <code>P</code> through a link called prototype. </p> <p>Each object in JS has a built-in property, called its prototype. Prototype itself is an object, which means it itself has its own prototype, making a prototype chain. This chain ends at <code>Object.prototype</code> which has <code>null</code> prototype:</p> <pre><code>const o = {};\n// __proto__ is a legacy feature, is not recommended.\no.__proto__;                             // Object.prototype\nObject.getPrototypeOf(o);                // Object.prototype\nObject.getPrototypeOf(Object.prototype); // null\n\nconst a = [];\nObject.getPrototypeOf(a);                // Array.prototype\nObject.getPrototypeOf(Array.prototype);  // Object.prototype\n</code></pre> <p> []Array.prototypeObject.prototype{}null </p>"},{"location":"js/types/","title":"Types","text":"<p>There are 8 basic types in JS, with all but Objects being primitive.</p>"},{"location":"js/types/#boolean","title":"Boolean","text":"<p>Straightforward (or so I thought), has two possible values: <code>true</code> or <code>false</code>. Following values convert to <code>false</code>:</p> <pre><code>undefined\nnull\n0\n-0\nNaN\n\"\" // empty string\n</code></pre> <p>all other values convert to <code>true</code>. Use <code>===</code>/<code>!==</code> to avoid the implicit type coercion. However, <code>[]</code> is both truthy and loosely <code>false</code>. It's truthy because all objects are truthy, but it's also <code>false</code> because it's converted to primitive via <code>toString()</code> and emits <code>\"\"</code>.</p> <pre><code>if ([]) {\n  // executes\n}\n</code></pre> <pre><code>if ([] == false) {\n  // executes too\n}\n</code></pre> <p>Use <code>Boolean</code> to convert non-boolean values to boolean. Don't use it with <code>new</code> operator as all objects are truthy, even <code>Boolean</code> objects.</p> <pre><code>const good = Boolean(expression);\n\nif (new Boolean(false)) { // Don't do this!\n  // this will execute\n}\n</code></pre>"},{"location":"js/types/#null-undefined","title":"null &amp; undefined","text":"<p>Both indicate the absence of a value. <code>==</code> consider them to be equal, falsy values.</p> <p><code>null</code> is the sole member of its own type and implies a program-level, intentional absence of a value. <code>typeof</code> will report it incorrectly as <code>'object'</code>, that's a legacy bug.</p> <p><code>undefined</code> represents uninitialized values, a deeper kind of a absence. <code>undefined</code> is variable in global scope.</p>"},{"location":"js/types/#symbol","title":"Symbol","text":"<p>Introduced in ES6 to serve as non-string property names. There is no literal syntax for symbols.</p> <p>When you want to keep your symbols private to your own code, guaranteed that they will not conflict with properties used by other code.</p> <p>When you want to share symbol widely with other code.</p> acquire unique symbols<pre><code>const a = Symbol();\nconst b = Symbol(\"foo\");\nconst c = Symbol(\"foo\");\n\na == b; // false\nb == c; // false\n</code></pre> acquire shared symbols<pre><code>const a = Symbol.for(\"shared\");\nconst b = Symbol.for(\"shared\");\n\na === b;          // true\nSymbol.keyFor(a); // \"shared\"\n</code></pre>"},{"location":"js/types/#string","title":"String","text":"<p>JS strings use UTF-16 encoding of Unicode character set as sequences of unsigned 16-bit values. This is fine for most commonly used unicode characters as they'll fit in a single code point. </p> <p>But there are unicode characters which will take two code points. These will report wrong <code>length</code> and attempts to iterate over these may give unexpected results. That's why it's recommended to use for/of loop.</p> wrong way to iterate<pre><code>const smiley = \"\ud83d\ude04\";\nsmiley.length; // 2, \\ud83d\\ude04\n\nfor (let i = 0; i &lt; smiley.length; i++)\n  console.log(smiley.charAt(i)); // \ufffd \ufffd\n</code></pre> right way to iterate<pre><code>const smiley = \"\ud83d\ude04\";\nlet length = 0;\n\nfor (let c of smiley) {\n  console.log(c); // \"\ud83d\ude04\"\n  length++;\n}\n\nconsole.log(length); // 1\n</code></pre> <p>String created from literal and from <code>String()</code> call are primitive values.</p> <p>But ones created from <code>new String()</code> are an object.</p> <pre><code>'test' // 'test'\n\"test\" // 'test'\n\nconst name = \"bob\";\n`hello\n${name}` // 'hello\\nbob', \n\nString(1);        // '1'\ntypeof String(1); // 'string'\n</code></pre> <pre><code>new String(\"foo\"); // String {'foo'}\ntypeof s;          // 'object'\n</code></pre>"},{"location":"js/types/#number-bigint","title":"Number &amp; BigInt","text":"<p>There are no distinct integer and floating representation, instead JS uses 64-bit IEEE 754 floating point for both. \\(1\\) bit for sign, \\(11\\) bits for exponent and \\(52\\) bits for mantissa (a number b/w \\(0\\) and \\(1\\)).</p> <p>\\[   \\text{number} = -1^\\text{sign} \\cdot (1 + \\text{mantissa}) \\cdot 2^{\\text{exponent}} \\]</p> <p>Integers can be represented without loss of precision in range \\([-2^{53}+1, 2^{53}-1]\\). Use <code>BigInt</code> beyond that.</p> <pre><code>Number.MAX_VALUE        // 1.7976931348623157e+308\nNumber.MAX_SAFE_INTEGER // 9007199254740991\nNumber.MAX_SAFE_INTEGER + 1 === Number.MAX_SAFE_INTEGER + 2 // true, precision loss\n\nconst num = 1234567890123456789012345678901234567890n;\ntypeof num; // bigint\n</code></pre>"},{"location":"js/types/#object","title":"Object","text":"<p>Object types (objects, arrays, functions) need their own section for a thorough breakdown.</p>"},{"location":"js/types/#typeof-heads-up","title":"<code>typeof</code> heads up","text":"<pre><code>typeof 0           // 'number'\ntypeof 1n          // 'bigint'\ntypeof true        // 'boolean'\ntypeof 'foo'       // 'string'\ntypeof undefined   // 'undefined'\ntypeof null        // 'object', known bug \ntypeof Symbol()    // 'symbol'\n\ntypeof {}          // 'object'\ntypeof console.log // 'function', still an object but treated differently by typeof\n</code></pre>"},{"location":"kt/concurrency/","title":"Coroutines","text":""},{"location":"kt/concurrency/#coroutines_1","title":"Coroutines","text":""},{"location":"kt/concurrency/#intro","title":"Intro","text":"<p>source and Kotlin in Action 2</p> <p>A coroutine is an instance of suspendable computation. Conceptually similar to a thread, but with a number of advantages:</p> <ol> <li>Unlike threads, cheap to make. One can run 100K or more coroutines on a bog standard laptop.</li> <li>Unlike threads, doesn't block system resources when suspended.</li> <li>Structured concurrency ensures that child coroutines don't hang around when parent coroutine is cancelled.</li> </ol> <p>Under the hood, coroutines are implemented by running on one or more JVM threads. Over its life, a coroutine may start on one thread, suspend, and then resume on another.</p> <pre><code>fun main() = runBlocking {\n  launch {\n    delay(1000L)\n    println(\"World!\")\n  }\n  print(\"hello \")\n}\n</code></pre> <p>this prints <code>Hello World!</code>. Let's break it down:</p> <ol> <li> <p><code>runBlocking</code> is a coroutine builder, a special one at that which bridges the regular blocking code with suspending functions. Without it, <code>launch</code> line will give the error <code>Unresolved reference: launch</code>. It's called <code>runBlocking</code> because the thread that runs it (i.e. main thread here) gets blocked for the duration of the call.</p> </li> <li> <p><code>launch</code> is another coroutine builder that launches a new coroutine concurrently with the rest of the code. That's why <code>Hello</code> gets printed first.</p> </li> <li> <p><code>delay</code> is a special suspending function that suspends the coroutine for the specified time. Unlike <code>Thread.sleep</code>, this does not block the underlying thread, and instead allows other coroutine to take turn.</p> </li> <li> <p>A suspending function can only be called from another suspending function or a coroutine.</p> </li> <li> <p><code>launch</code> is meant for starting coroutines that don't return value. Actually, <code>launch</code> returns a <code>Job</code>, think of a <code>ListenableFuture&lt;Void&gt;</code>.</p> </li> <li> <p>If you want a result from a coroutine, start it with <code>async</code> which returns a <code>Deferred&lt;T&gt;</code>.</p> </li> </ol> <p>Another example:</p> <pre><code>fun main() = runBlocking {\n  println(\"parent starts\")                 // 1\n\n  launch {\n    println(\"second coroutine\")            // 3\n    delay(100.milliseconds)\n    println(\"second coroutine resumed\")    // 5\n  }\n\n  launch {\n    println(\"third coroutine\")             // 4\n  }\n\n  println(\"parent launched 2 coroutines\")  // 2\n}\n</code></pre> <pre><code>parent starts\nparent launched 2 coroutines\nsecond coroutine\nthird coroutine\nsecond coroutine resumed\n</code></pre>"},{"location":"kt/concurrency/#dispatchers","title":"Dispatchers","text":"<pre><code>launch(Dispatchers.Default) {\n  // do something on default dispatcher\n}\n</code></pre> <p>Dispatcher determines what thread(s) the coroutine uses for its execution. Dispatchers are inherited by child corountines from their parents. Following dispatchers are available out of the box:</p> Dispatcher Number of threads Used for <code>Dispatchers.Default</code> Number of CPU cores General purpose CPU-bound <code>Dispatchers.Main</code> One UI work <code>Dispatchers.IO</code> Auto-scaling  \\(\\text{max}(\\text{CPU cores}, 64)\\) Blocking IO task <code>Dispatchers.Unconfined</code> Any Used for immediate scheduling <code>limitedParallelism(n)</code> <code>n</code> Custom scenarios <ul> <li><code>Unconfined</code> executes coroutine immediately on current thread and later resumes it whatever thread called <code>resume</code>.</li> <li><code>limitedParallelism(n)</code> creates a view of the current dispatcher but with guarantee that no more than <code>n</code> coroutines are executed at any time.</li> </ul>"},{"location":"kt/concurrency/#launch-and-async","title":"<code>launch</code> and <code>async</code>","text":"<pre><code>public fun CoroutineScope.launch(\n  context: CoroutineContext = EmptyCoroutineContext,\n  ...\n): Job { ... }\n</code></pre> <pre><code>public fun &lt;T&gt; CoroutineScope.async(\n  context: CoroutineContext = EmptyCoroutineContext,\n  ...\n): Deferred&lt;T&gt; { ... }\n</code></pre> <p>Both coroutine builders are actually extension of <code>CoroutineScope</code>. Well, what is a <code>CoroutineScope</code>?</p>"},{"location":"kt/concurrency/#coroutinescope","title":"CoroutineScope","text":"<pre><code>public interface CoroutineScope {\n  public val coroutineContext: CoroutineContext\n}\n</code></pre> <p>It's just a wrapper arount <code>CoroutineContext</code>. And what is that?</p>"},{"location":"kt/concurrency/#coroutinecontext","title":"CoroutineContext","text":"<pre><code>// Persistent context for the coroutine. It is an indexed set of [Element] instances.\npublic interface CoroutineContext {\n  public operator fun &lt;E : Element&gt; get(key: Key&lt;E&gt;): E?\n}\n\n...\n\npublic interface Element : CoroutineContext {\n  public val key: Key&lt;*&gt;\n  ...\n}\n</code></pre> <p>It's an indexed set of <code>Element</code>s, each of which are <code>CoroutineContext</code> themselves, exemplified in following snippet:</p> <pre><code>fun main() {\n  val foo: CoroutineContext = CoroutineName(\"foo\")\n  println(foo[CoroutineName]) // CoroutineName(foo)\n  println(foo[Job]) // null\n\n  val bar: CoroutineContext = CoroutineName(\"bar\")\n  println(bar[CoroutineName]) // CoroutineName(bar)\n  println(foo[Job]) // null\n\n  println((foo + bar)[CoroutineName]) // CoroutineName(bar)\n  println((bar + foo)[CoroutineName]) // CoroutineName(foo)\n}\n</code></pre> <p>P.S. <code>EmptyCoroutineContext</code> seen before has no elements, as the name implies.  </p> <p>That is, coroutine context is a set of \"elements\". The main elements are <code>Job</code> and the dispatcher.</p> <pre><code>fun main() {\n  val foo: CoroutineContext = Dispatchers.Default + Job()\n  println(foo[CoroutineName]) // null\n  println(foo[Job]) // JobImpl{Active}@5c29bfd\n  println(foo[Job]?.isActive) // true\n\n  foo.job.cancel()\n\n  println(foo[Job]?.isActive) // false\n}\n</code></pre>"},{"location":"kt/concurrency/#adding-it-all-together","title":"Adding it all together","text":"<p>Source - Roman Elizarov: Kotlin Project Lead</p> <p>Recall our prior example of launching a coroutine with a given dispatcher:</p> <pre><code>fun main() = runBlocking {\n  launch(Dispatchers.IO + Job()) {  // Passed context = [IO + Job()]\n  }\n}\n</code></pre> <p><code>launch</code>/<code>async</code> is passed a <code>CoroutineContext</code> as parameter. But it also is an extension of <code>CoroutineScope</code> which itself is wrapping a context. That is, a coroutine builder is actually taking in two contexts.</p> <p>The two contexts are merged, with elements of passed in context taking precedence over the elements of implicit (scoped) context. This is the parent context of the new coroutine. The child context is this <code>parent context</code> \\(+\\) a new <code>Job()</code> (child of parent job).</p> <p> Scope JobScope ContextScope dispatcherParent ScopePassed JobPassed ContextPassed dispatcher.launch()+Parent JobParent ContextParent dispatcher= Scope Job Passed Job= Parent dispatcher Passed dispatcherChild Job = Job()Child ContextParent DispatcherChild ScopePassed context overwrites elementschild context = parent context + Job()</p> <p>We can see it all in work like this:</p> <pre><code>fun main() = runBlocking {\n  launch(Dispatchers.Default) {\n    val scopeContext = currentCoroutineContext()      // [StandaloneCoroutine{Active}@7d6b54ea, Dispatchers.Default]\n    val scopeJob = scopeContext.job                   //  StandaloneCoroutine{Active}@7d6b54ea\n\n    val parentContext = scopeContext + Dispatchers.IO // [StandaloneCoroutine{Active}@7d6b54ea, Dispatchers.IO]\n\n    launch(Dispatchers.IO) {\n      val childContext = currentCoroutineContext()    // [StandaloneCoroutine{Active}@4bcf3652, Dispatchers.IO]\n      val childJob = childContext.job                 //  StandaloneCoroutine{Active}@4bcf3652\n\n      println(\"${childContext == parentContext + childJob}\") // true\n      println(\"${childJob.parent == scopeJob}\")              // true \n    }\n  }\n}\n</code></pre> <p>Had we passing in <code>Dispatcher.IO + Job()</code> to <code>launch</code>, the <code>childJob</code> would have been child of this new <code>Job</code> instead.</p>"},{"location":"kt/concurrency/#coroutinescope_1","title":"<code>coroutineScope</code>","text":"<p>Source</p> <p>In addition to coroutine scope provided by different builders, it is possible to create your own scope using <code>coroutineScope</code>. It creates a new scope and does not complete until all launched children complete.</p> <p>It looks similar to <code>runBlocking</code>, but <code>runBlocking</code> blocks current thread while <code>coroutineScope</code> just suspends. That's why <code>corountineScope</code> is a suspend function whereas <code>runBlocking</code> is a regular function.</p> <p><code>coroutineScope</code> will fail if any of the child coroutine fail, resulting in cancellation of all other children. If you want other children to still complete regardless, use <code>supervisorScope</code>.</p> <pre><code>fun main() = runBlocking {\n  coroutineScope {\n    launch {\n      delay(5000L)\n      println(\"a\")\n    }\n    launch {\n      delay(1L)\n      throw Exception()\n    }\n  }\n  println()\n}\n</code></pre> <pre><code>fun main() = runBlocking {\n  supervisorScope {\n    launch {\n      delay(5000L)\n      println(\"a\")\n    }\n    launch {\n      delay(1L)\n      throw Exception()\n    }\n  }\n  println()\n}\n</code></pre> <p>finishes in right away</p> <p>finishes after 5 second </p>"},{"location":"kt/concurrency/#cancelling-coroutines","title":"Cancelling coroutines","text":"<p>Kotlin in Action 2</p> <p>We can call <code>cancel</code> on both <code>Job</code> and <code>Deferred&lt;T&gt;</code> to cancel the coroutine ahead of its time. This is useful in avoiding unnecessary work and leaks. </p> <p>Cancellation of parent cadcades through all children coroutines. Note that this is different thing from <code>coroutineScope/supervisorScope</code> which is about sibling cancellation.</p> <pre><code>fun main() = runBlocking {\n  val job = launch {\n    launch {\n      launch {\n        launch {\n          delay(1000L)\n          println(\"descendant 1 done\") // prints\n        }\n        launch {\n          delay(10_000L)\n          println(\"descendant 2 done\") // does not print\n        }\n      }\n    }\n  }\n  delay(1500L)\n  job.cancel()\n}\n</code></pre> <p>this exits right after 1.5s.</p> <p>We can also use <code>withTimeout</code> and <code>withTimeoutOrNull</code> to automatically cancel a coroutine.</p> <pre><code>fun main() = runBlocking {\n  withTimeout(50_000L) {\n    delay(10_000L)\n    println(\"hello\")\n  }\n}\n</code></pre> <pre><code>fun main() = runBlocking {\n  withTimeout(500L) {\n    delay(10_000L)\n    println(\"hello\")\n  }\n}\n</code></pre> <p>prints <code>hello</code> after 10s</p> <p>aborts after .5s</p> <p>Cancellation is implemented by throwing a special exception type <code>CancellationException</code>. So never do a catch-all for <code>CancellationException</code> or its super-type <code>IllegalStateException</code>, <code>RuntimeException</code>, <code>Exception</code> and <code>Throwable</code>. Otherwise you will prevent the cancellation.</p>"},{"location":"kt/concurrency/#suspension-point","title":"Suspension Point","text":"<p>source 1 and 2.</p> <p>A suspending function is different from non-suspending functions by having zero or more suspension points - statements in the body where the function execution can be paused and resumed later. Usually that's the points at which the function calls other suspending functions. </p> <p>That's why non-suspending functions cannot call suspending ones, as they do not support suspension points. Also, suspending functions may call non-suspending functions knowing that it will not introduce a suspension point. See function coloring.</p> <pre><code>coroutineScope {\n  print(\"A\")\n  delay(500L) &lt;- suspension point\n  print(\"B\")\n  print(\"C\")\n}\n</code></pre> <p>this code will either print <code>A</code> or <code>ABC</code>, but never <code>AB</code> as there are no suspension point between <code>B</code> and <code>C</code>. </p> <p>Why are we talking about this? Consider the following example:</p> <pre><code>fun heavyCpuWork(): Int {\n  while(...) {\n    compute()\n  }\n}\n\nfun main() = runBlocking {\n  launch { heavyCpuWork() }\n  launch { heavyCpuWork() }\n}\n</code></pre> <p>Just using coroutines will not magically make our code concurrent. In the example above, <code>heavyCpuWork</code> has no suspension point. As a result, even when running on a coroutine, it will never suspend to let other coroutine have a go. The first corountine will finish to completion before second one gets a chance.</p> <pre><code>fun heavyCpuWork(): Int {\n  while(...) {\n    compute()\n    yield()\n  }\n}\n</code></pre> <pre><code>fun heavyCpuWork(): Int {\n  while(...) {\n    if (isActive) {\n      compute()\n    }\n  }\n}\n</code></pre> <p>Use <code>yield</code> function lets coroutine voluntarily give way for other corountines.</p> <p>The other approach is to explicitly check cancellation status.</p>"},{"location":"kt/concurrency/#channels","title":"Channels","text":"<p>Source</p> <p>Basically <code>BlockingQueue</code> but instead of a blocking <code>put</code>, it has a suspending <code>send</code> and instead of a blocking <code>take</code>, it has a suspending <code>receive</code>.</p> <pre><code>suspend fun produce(channel: Channel&lt;Int&gt;) {\n  repeat(20) {\n    delay(1000L)\n    channel.send(it)\n  }\n  channel.close()\n}\n\nsuspend fun consume(channel: Channel&lt;Int&gt;) {\n  for (i in channel) { // until channel is closed\n    println(\"${coroutineContext[CoroutineName]?.name} received $i\")\n  }\n}\n\nfun main() = runBlocking {\n  val channel = Channel&lt;Int&gt;()\n  launch(CoroutineName(\"A\")) { consume(channel) }\n  launch(CoroutineName(\"B\")) { consume(channel) }\n  launch(CoroutineName(\"C\")) { consume(channel) }\n  launch(CoroutineName(\"D\")) { consume(channel) }\n  launch(CoroutineName(\"P\")) { produce(channel) }\n  println()\n}\n</code></pre> <pre><code>A received 0\nB received 1\nC received 2\nD received 3\nA received 4\nB received 5\nC received 6\nD received 7\nA received 8\nB received 9\nC received 10\nD received 11\nA received 12\nB received 13\nC received 14\nD received 15\nA received 16\nB received 17\nC received 18\nD received 19\n</code></pre> <p>The example above had no buffer. Unbuffered channels transfer elements when sender and receiver meet each other (i.e. rendezvous). If sender is invoked first, then it gets suspended until receiver is invoked, and vice-versa.</p> <p>We can optionally specify as capacity <code>Channel&lt;T&gt;(42)</code> which allows sender to send multiple items before getting suspended.</p>"},{"location":"kt/concurrency/#flow","title":"Flow","text":"<p>Flow</p> <p><code>Flow</code> is basically <code>Sequence</code> that also support suspending functions. And a sequence is like <code>Stream</code>, but more kotliny.</p> <pre><code>fun produce() = sequence {\n  for (i in 1..5) {\n    Thread.sleep(1000L)\n    yield(i)\n  }\n}\n\nfun main() {\n  produce().forEach { println(it) }\n}\n</code></pre> <pre><code>fun produce() = flow {\n  for (i in 1..5) {\n    delay(1000L)\n    emit(i)\n  }\n}\n\nfun main() = runBlocking {\n  produce().collect { println(it) }\n}\n</code></pre> <p>Prints <code>1</code>, <code>2</code>, <code>3</code>, <code>4</code>, <code>5</code>, each after 1s delay.</p> <p>Does the same thing, but doesn't block the thread, just suspends it.</p> <p>Notice that the function is no longer marked <code>suspend</code>, why's that? See <code>suspend</code> makes sense when function is returning just one thing and that thing may take time to \"cook\". <code>Flow</code>, on the other hand, is a lightweight object that's available right away. The function will always return it immediately. So a <code>suspend foo(): Flow</code> is moot in the same manner <code>suspend foo(): ListenableFuture&lt;T&gt;</code> is moot.</p> <p>Once you got your hands on a flow, then sure it may take time to collect items from it. Which is why <code>collect</code> is still suspending function. </p> <p>Flows are cold streams similar to sequences, i.e. the builder does not run until the flow is <code>collect</code>ed. And you can do the same things to Flows as you can do with Sequences/Streams.</p> <p>By default, code in <code>flow {...}</code> runs in the context that is provided by its collector. But it can be changed like this:</p> <pre><code>flow {\n  ...\n}.flowOn(Dispatchers.Default)\n</code></pre> <pre><code>fun produce() = flow {\n  for (i in 1..5) {\n    delay(1000L)\n    emit(i)\n  }\n}\n\nfun main() = runBlocking {\n  produce()\n  .collect {\n    delay(3000L)\n    println(it)\n  }\n}\n</code></pre> <pre><code>fun produce() = flow {\n  for (i in 1..5) {\n    delay(1000L)\n    emit(i)\n  }\n}\n\nfun main() = runBlocking {\n  produce()\n  .buffer()\n  .collect {\n    delay(3000L)\n    println(it)\n  }\n}\n</code></pre> <p>Takes 4s to print each number (1s + 3s) due to running emit and collect sequentially.</p> <p><code>buffer</code> runs emitting code of <code>produce</code> concurrently with collecting code. So we wait 1s for first number and then only 3s each number.</p> <p>Similar to <code>buffer</code>, use <code>conflate</code> to skip intermediate steps when the collector is too slow and you want to drop in-between emitted values. For <code>xxx</code> operator like <code>collect</code>, there is a family of <code>xxxLatest</code> operators (e.g. <code>collectLatest</code>) that cancel their code on a new value.</p> <p>We can use <code>catch</code> before <code>collect</code> to grab exceptions thrown by the emitter. Again, note that the <code>collect</code> is not same as <code>collect</code> of <code>Stream</code>s. Without <code>collect</code>, a flow is cold, never emitting a value.</p>"},{"location":"kt/concurrency/#hot-flows","title":"Hot Flows","text":"<p>Hot flows are useful when sharing emitted items across multiple collectors, this time called subscribers. Which is why it doesn't make sense for relying on a collector to start collecting. </p> <p>It's like broadcasting. A TV station or a twitch stream will broadcast, regardless of whether there are any viewers or not.</p> <p>Kotlin has two implementations of hot flows out of the box: Shared flows and State flows.</p> producer<pre><code>val flow = MutableSharedFlow&lt;Int&gt;()\n\nfun broadcast(scope: CoroutineScope) = scope.launch {\n  repeat(1000) {\n    delay(1000L)\n    flow.emit(\"ping\")\n  }\n} \n</code></pre> subscriber<pre><code>val readOnly = flow.asSharedFlow()\n\nreadOnly.collect{ println(it) }\n</code></pre> <p>Subscribers only receive emissions that happened after they called <code>collect</code>. A hot flow can let you see some past broadcast with <code>replay</code> param: <code>MutableSharedFlow&lt;Int&gt;(replay = 10)</code>.</p> <p>A special case of hot flow is when you need to track state of a system right now and don't care for past values.</p> <pre><code>private val viewWriter = MutableStateFlow(0) // inital views = 0\nprivate vale viewReader = viewWriter.asStateFlow() \n\nfun increment() {\n  viewWriter.update { it + 1 }\n}\n\nfun main() {\n  increment()\n  println(reader.value)\n}\n</code></pre> <p><code>StateFlow</code> only emit last known value, so no <code>relay</code>. They also come with a start value in the constructor that's emitted immediately. In general, it's a simpler API.</p>"},{"location":"misc/display/","title":"Display Upgrade","text":""},{"location":"misc/display/#lore","title":"Lore","text":"<p>I used to daily drive Asus Vivobook Pro 14X OLED (M7400QC) as my personal laptop. This is a great laptop, but in just two years of owning it, I started to feel a number of pain points:</p> <ol> <li>14 inch display was just too small. Especially if you are multi-windowing like me.</li> <li>No thunderbolt port. So connecting to my external display is a chore.</li> <li>I can't use Macbook chargers that are laying around. It's own 120W brick is indeed a brick (weight-wise).</li> <li>No expansion option. It has 16GB RAM soldered.</li> </ol> <p>At the time of writing this, the best option was another Asus machine, Asus Vivobook 16X. It ticked all the boxes of what I needed, and more:</p> <ul> <li>[x] Thunberbolt port.</li> <li>[x] 16 inch display.</li> <li>[x] Expandable memory.</li> <li>[x] RTX 4060</li> </ul> <p>In my haste to move on with my life, I didn't look too much into it and ordered one online. Only to discover later that the display is 1960x1200 (WUXGA?). </p>"},{"location":"misc/display/#the-downgrade","title":"The Downgrade","text":"<p>My previous laptop had an OLED display, 2880x1800 display size with 242PPI. For comparison, the new laptop is just 141PPI. That's a substantial downgrade, and you feel it. Within 15 mins of booting the laptop up, I knew something was up. </p> <p>It turns out, Asus has bunch of SKUs with similar name. There are Vivobook, Vivobook S, Vivobook Pro. I had bought one without OLED screen (which I don't care for much), but more crucially, a very low res screen.</p>"},{"location":"misc/display/#the-upgrade","title":"The Upgrade","text":"<p>Luckly, I stumbled on this video from NL Tech. Don't remember what I was searching for, but thanks Google!</p> <p>I followed the guide, and found that new laptop is using this panel NE160WUM-NX1 from HWiNFO.</p>"},{"location":"misc/display/#metrics-display-quality","title":"Metrics: display quality","text":"<p>The specs we are concerned are these:</p> <ul> <li>Panel Type: Oxide TFT-LCD</li> <li>Resolution: 1920x1200, 141 PPI</li> <li>Viewing angle: 89/89/89/89</li> <li>Contrast Ratio: 1200:1</li> <li>Adobe Coverage: 74%</li> <li>DCI-P3 Overage: 74%</li> <li>Brightness: 300 \\(cd/m^2\\)</li> </ul> <p>Write these down somewhere. The idea is to make sure we don't end up getting a worse panel. Higher screen resolution is great, but that shouldn't come at the expense of these other metrics. What do they all mean?</p> <ul> <li>Panel Type: that's LCD, OLED etc. I had past experience with IPS panels, so I knew to keep a watch out for this. In short, go for OLED if possible and LCD if not.</li> <li>Viewing angle: the IPS panel I mentioned had a viewing angle of ~45 deg. You'd know when you see one. Just slightly tilting the panel will make reading the screen impossible, with the background lights bleeding through.</li> <li>Contrat: that should be obvious. Poor contrast means washed out colors.</li> <li>Coverage %: this is not that obvious, but think of this as panel adding a bias/tint to everything you see.</li> <li>Brightness: higher the better, otherwise you'll struggle using it in a lit room.</li> </ul> <p>The current panel was pretty good when it comes to all these metrics (other than resolution). So I knew the baseline number to look for.</p>"},{"location":"misc/display/#metrics-interface","title":"Metrics: interface","text":"<p>Now we want to find out about the compatible panels we replace our current display with. Panelook shows this under <code>Signal Type</code> on the Specs tab. For me, this was:</p> <ul> <li>Signal type: eDP, 4 lane, eDP 1.4</li> <li>Pins: 40</li> <li>Interface model: <code>MSAK24025P40G</code> (optional)</li> <li>Interface position: Panelook showed this for my current panel .</li> </ul> <p>We want to search for panels with same number of pins (40) and same interface type (eDP 1.4). To be sure without a shred of doubt, you can also limit yourself to panels using the same interface model. Though, this turned out not to be necessary in my case.</p>"},{"location":"misc/display/#result","title":"Result","text":"<p>In fact, I order two different panels:</p> <ul> <li>NE160QDM-NYD: a 2560x1660 panel which used the exact same <code>MSAK24025P40G</code> connector and had it placed in the exact same spot.</li> <li>NE160QAM-NZ1: a 3840x2400 panel which was best option all around, but didn't have the same connector and had slighly different placement.</li> </ul> <p>You can actually see the comparison between the three panels (stock, NE160QDM-NYD, NE160QAM-NZ1) here.</p> <p>I'm now using <code>NE160QAM-NZ1</code>, and it's great!. Display quality-wise, this is what the upgrade looks like:</p> <ul> <li>Panel Type: Oxide TFT-LCD (unchanged)</li> <li>Resolution: 3840x2400, 282PPI (double the pixels)</li> <li>Viewing angle: 89/89/89/89 (unchanged)</li> <li>Contrast Ratio: 1200:1 (unchanged)</li> <li>Adobe Coverage: 88% (+14%)</li> <li>DCI-P3 Overage: 100% (+26)</li> <li>Brightness: 500 \\(cd/m^2\\) (+200)</li> </ul> <p>Even up close, my eyes can't discern any two pixels apart. It's more vibrant, more bright. There is enough room now on screen to tile 4 different windows on each corner.</p>"},{"location":"misc/display/#tools-needed","title":"Tools needed","text":"<p>iFixit Pro toolkit had all that I needed, which was just the screendriver bits, plastic opening picks (for laptop case), jimmy (for display panel) and tweezer (for adhesive rubber). I used the OSFT 2mm double sided tape to glue the new panel.</p>"},{"location":"misc/display/#panel-purchase","title":"Panel purchase","text":"<p>Just search the panel model online and you'd stumble onto a number of websites.</p> <ul> <li>Ubuy: I bought the 2560 panel from here. Don't use it. I ended up paying ~50% surchage on customs, and they sent the wrong panel.</li> <li>Bliss computers: very slow processing. </li> <li>eBay: that's where I bought the 3840 panel from win0win, really helpful folks. Delivered within 15 days, no customs and actually cheaper than 2560 model. </li> </ul>"},{"location":"misc/display/#extras","title":"Extras","text":"<p>During laptop disassembly, take pictures at each steps. Really useful when putting it back together. Also check for videos on YT where people are opening your laptop model (or same brand at least), this is helpful in watching out for OEM quirks.</p>"},{"location":"numbers/arithmetic/","title":"Bit arithmetic","text":""},{"location":"numbers/arithmetic/#rightmost-bits","title":"Rightmost bits","text":"<p><code>x &amp; (x-1)</code> to unset rightmost 1-bit in a word.</p> <pre><code>       x  = aaaa 1000\n     x-1  = aaaa 0111\nx &amp; (x-1) = aaaa 0000 \n</code></pre> <p><code>x</code> will have only one bit set if it's a power of two, meaning that <code>x &amp; (x-1) == 0</code> for \\(x = 2^n\\).</p> <p><code>x | (x+1)</code> to set rightmost 0-bit in a word. </p> <pre><code>       x  = aaaa 0111 \n     x+1  = aaaa 1000\nx | (x+1) = aaaa 1111 \n</code></pre> <p><code>x &amp; (x+1)</code> to unset trailing 1's in a word. </p> <pre><code>       x  = aaaa 0111\n     x+1  = aaaa 1000\nx &amp; (x+1) = aaaa 0000\n</code></pre> <p><code>x &amp; -x</code> to isolate rightmost 1-bit. </p> <pre><code>     x = 0000 1000  (8)\n    -x = 1111 1000 (-8)\nx &amp; -x = 0000 1000\n</code></pre> <p><code>x &lt;&lt; n | x &gt;&gt; (32-n)</code> to left shift <code>x</code> by <code>n</code> steps.</p> <p> 132nn3232&lt;&lt;n1&gt;&gt; (32-n)132</p> <p><code>x &gt;&gt; n | x &lt;&lt; (32-n)</code> to right shift <code>x</code> by <code>n</code> steps. </p> <p><code>x = a ^ b ^ x</code> to alternate between <code>a</code> and <code>b</code>, same as:</p> <pre><code>x == a ? b : a\n\na ^ b ^ a = b\na ^ b ^ b = a\n</code></pre>"},{"location":"numbers/representation/","title":"Number representation","text":"<p>A signed 32-bit integer can represent the range \\([-2^{31}, 2^{31}-1]\\). An unsigned integer need not allocate the sign bit and thus represents \\([0, 2^{32}-1]\\).</p>"},{"location":"numbers/representation/#2s-complement","title":"2's complement","text":"<p>(source)</p> <p>The method of representing signed integers on computers.</p> <ol> <li>A positive number \\(v\\) will have MSB as <code>0</code> and remaining \\(n-1\\) bits as \\(v\\).</li> <li>a negative number \\(v\\) will have MSB as <code>1</code> and remaining \\(n-1\\) bits as \\(\\overline{|v|} + 1\\). That is:<ol> <li>First take the binary representation of \\(|v|\\),</li> <li>then invert all its bits,</li> <li>and then add \\(1\\) while ignoring any overflow.</li> </ol> </li> </ol> <p>Example: 2's complement for 8-bit integer <pre><code>decimal          binary\n 0               0000 0000\n\n 1               0000 0001\n 2               0000 0010\n 3               0000 0011\n 4               0000 0100\n 5               0000 0101\n\n-1               1111 1111\n-2               1111 1110\n-3               1111 1101\n-4               1111 1100\n-5               1111 1011        \n</code></pre></p> <p>more examples</p>"},{"location":"numbers/representation/#convert-to-binary","title":"Convert to binary","text":"<p><code>%b</code> formatter is supported in most languages to get the number in binary format. However, for negative number it will simply print the binary representation of absolute value with <code>-</code> prefix. So we need a custom function:</p> <pre><code>// Can't declare method on int directly.\ntype Int int\n\nfunc (n Int) ToBinary(bits int) string {\n  // we know ahead of times how many runes we need\n  b := make([]rune, bits)\n\n  for i := 0; i &lt; bits; i++ {\n    if n &amp; (1 &lt;&lt; i) == 0 {\n      // MSB is at index 0, LSB is at index (length - 1)\n      b[bits - 1 - i] = '0'\n    } else {\n      b[bits - 1 - i] = '1'\n    }\n  }\n\n  return string(b)\n}\n\nfunc main() {\n  for _, v := range []Int{0, 1, 2, 3, 4, 5, -1, -2, -3, -4, -5} {\n    fmt.Printf(\"%2d as %s\\n\", v, v.ToBinary( /* should be strconv.IntSize */ 8))\n  }\n}\n</code></pre>"},{"location":"numbers/representation/#arithmetics","title":"Arithmetics","text":"Subtracting a positive numberAdding two positive numbersAdding two negative numbers <pre><code>15 - 9 = 15 + (-9)\n\n15 = 0000 1111\n-9 = 1111 0111\n 6 = 0000 0110\n</code></pre> <p>is same as adding a negative number</p> <pre><code>15 + 9\n\n15 = 0000 1111\n 9 = 0000 1001\n24 = 0001 1000\n</code></pre> <pre><code>-15 - 9\n\n-15 = 1111 0001\n -9 = 1111 0111\n-24 = 1110 1000\n</code></pre>"},{"location":"numbers/representation/#ieee-754","title":"IEEE 754","text":"<p>(source)</p> <p>a floating point is represented in 3 parts:</p> <p>\\[ \\text{number} = -1^{sign} \\cdot (1 + \\text{fraction}) \\cdot 2^{exponent}   \\]</p> <ol> <li>sign bit, as usual takes 1 bit (msb). <code>0</code> will result in \\(-1^0 = 1\\) and <code>1</code> will result in \\(-1^1 = -1\\).</li> <li>exponent takes 8 bits in single-precision float and 11 bits in double-precision floating point number.</li> <li>fraction takes the remaining 23 / 52 bits.</li> </ol> <p> signexponentfraction1-bit8/11-bits23/52-bits</p> single-precisiondouble-precision <ol> <li>If exponent \\( \\in [1, 254]\\), the number is in normalized form where the exponent has a bias / excess of \\(127\\) and range \\([-126, 127]\\). \\[\\text{number} = -1^{sign} \\cdot (1 + \\text{fraction}) \\cdot 2^{exponent \\color{red} - 127} \\]</li> <li>If exponent is \\(0\\), then number is de-normalized. Exponent has a very small fixed value of \\(2^{-126}\\) which is needed to represent very small numbers, including 0. \\[\\text{number} = -1^{sign} \\cdot (0 + \\text{fraction}) \\cdot 2^{- 126} \\]</li> <li>If exponent is \\(255\\), it represents special values of <code>NaN</code> and \\(\\pm\\infty\\).</li> </ol> <p>can represent integers without loss of precision in range \\([-2^{24}, 2^{24}]\\).</p> <ol> <li>If exponent \\( \\in [1, 2046]\\), the number is in normalized form where the exponent has a bias / excess of \\(1023\\) and range \\([-1022, 1023]\\). \\[\\text{number} = -1^{sign} \\cdot (1 + \\text{fraction}) \\cdot 2^{exponent \\color{red} - 1023} \\]</li> <li>If exponent is \\(0\\), then number is de-normalized. Exponent has a very small fixed value of \\(2^{-1022}\\) which is needed to represent very small numbers, including 0. \\[\\text{number} = -1^{sign} \\cdot (0 + \\text{fraction}) \\cdot 2^{- 1022} \\]</li> <li>If exponent is \\(2047\\), it represents special values of <code>NaN</code> and \\(\\pm\\infty\\).</li> </ol> <p>can represent integers without loss of precision in range \\([-2^{53}, 2^{53}]\\).</p>"},{"location":"partition/partition/","title":"Partitioning","text":""},{"location":"partition/partition/#two-way-partition","title":"Two-way partition","text":"pseudocodekotlintests <pre><code>parition(nums):\n  cursor = 0, pivot  = nums[0]\n\n  for i in [1, nums.size):\n    if nums[i] &lt; pivot:\n      swap(cursor++, i)\n\n  swap(0, cursor - 1)\n</code></pre> <pre><code>// returns the index of pivot\nfun MutableList&lt;Int&gt;.partitionTwoWay(pivotIndex: Int = 0): Int {\n  if (size &lt; 2) return size-1\n  if (pivotIndex != 0) swap(0, pivotIndex)\n\n  var cursor = 1; val pivot = this[0]\n\n  for (i in 1..&lt;size)\n    if (this[i] &lt; pivot)\n      swap(cursor++, i)\n\n  cursor-- // cursor was left in second half\n  swap(cursor, 0)\n  return cursor\n}\n\nfun &lt;T&gt; MutableList&lt;T&gt;.swap(i: Int, j: Int) {\n  val t = this[i]\n  this[i] = this[j]\n  this[j] = t\n}\n</code></pre> <pre><code>package partition\n\nimport org.junit.jupiter.api.Assertions.*\nimport org.junit.jupiter.api.Test\nimport java.util.concurrent.ThreadLocalRandom\nimport kotlin.streams.toList\n\nclass PartitionTest {\n\n  private lateinit var list: MutableList&lt;Int&gt;\n\n  @Test\n  fun testEmpty() {\n    list = mutableListOf()\n    list.partitionTwoWay()\n    assertEquals(listOf&lt;Int&gt;(), list)\n  }\n\n  @Test\n  fun testSingle() {\n    list = mutableListOf(9)\n    list.partitionTwoWay()\n    assertEquals(listOf(9), list)\n  }\n\n  @Test\n  fun testDouble() {\n    list = mutableListOf(1, 2)\n    list.partitionTwoWay()\n    assertEquals(listOf(1, 2), list)\n\n    list = mutableListOf(2, 1)\n    list.partitionTwoWay()\n    assertEquals(listOf(1, 2), list)\n  }\n\n  @Test\n  fun testTriple() {\n    list = mutableListOf(1, 2, 3)\n    list.partitionTwoWay(1)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(1, 3, 2)\n    list.partitionTwoWay(2)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(2, 1, 3)\n    list.partitionTwoWay(0)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(2, 3, 1)\n    list.partitionTwoWay(0)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(3, 1, 2)\n    list.partitionTwoWay(2)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(3, 2, 1)\n    list.partitionTwoWay(1)\n    assertEquals(listOf(1, 2, 3), list)\n  }\n\n  @Test\n  fun fuzzy() {\n    val nums = ThreadLocalRandom.current().ints(100, 0, 80).toList().toMutableList()\n    val copy = nums.stream().toList().toMutableList().sorted()\n\n    nums.sortByPartition()\n\n    assertEquals(copy, nums)\n  }\n\n  // basically quicksort\n  private fun MutableList&lt;Int&gt;.sortByPartition() {\n    if (size &lt; 2) return\n\n    val pivotIndex = partitionTwoWay()\n\n    val left = this.subList(0, pivotIndex).toMutableList()\n    val right = this.subList(pivotIndex + 1, size).toMutableList()\n\n    left.sortByPartition()\n    right.sortByPartition()\n\n    for (i in 0..&lt;left.size)\n      this[i] = left[i]\n    for (i in 0..&lt;right.size)\n      this[pivotIndex + i + 1] = right[i]\n  }\n}\n</code></pre> <p> ppat the start&lt; p&gt;= pbefore the final swapcursor0 1 ...p&lt; p&gt;= pafter the final swapcursor - 1</p> <p>We start the <code>cursor</code> at index <code>1</code>, moving all the elements \\(\\lt\\) <code>pivot</code> in the first partition.   At the end of the loop, <code>cursor</code> will be left pointing at the start of second partition, which contains elements \\(\\ge\\) <code>pivot</code>.   So we take a step back (<code>cursor--</code>) and move the pivot at the end of the first partition.</p>"},{"location":"partition/partition/#three-way-partition","title":"Three-way partition","text":"pseudocodekotlintests <pre><code>parition(nums, pivot):\n  l = 0, c = 0, r = nums.length - 1\n\n  while c &lt;= r:\n    switch(nums[c]):\n      &lt; pivot -&gt; swap(l++, c++)\n      = pivot -&gt; c++\n      &gt; pivot -&gt; swap(c, r--)\n\n  return (l, c)\n</code></pre> <pre><code>fun MutableList&lt;Int&gt;.partitionThreeWay(pivot: Int = this[0]): Pair&lt;Int, Int&gt; {\n  var l = 0; var c = 0; var r = size - 1\n  while (c &lt;= r)\n    when {\n      this[c]  &lt; pivot -&gt; swap(l++, c++)\n      this[c] == pivot -&gt; c++\n      this[c]  &gt; pivot -&gt; swap(c, r--)\n    }\n  return Pair(l, c)\n}\n\nfun &lt;T&gt; MutableList&lt;T&gt;.swap(i: Int, j: Int) {\n  val t = this[i]\n  this[i] = this[j]\n  this[j] = t\n}\n</code></pre> <pre><code>import org.junit.jupiter.api.Assertions.*\nimport org.junit.jupiter.api.Test\nimport java.util.concurrent.ThreadLocalRandom\nimport kotlin.streams.toList\n\nclass PartitionTest {\n\n  @Test\n  fun threeWaySimplest() {\n    var list = mutableListOf(1, 2, 3)\n    list.partitionThreeWay(2)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(1, 3, 2)\n    list.partitionThreeWay(2)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(2, 1, 3)\n    list.partitionThreeWay(2)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(2, 3, 1)\n    list.partitionThreeWay(2)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(3, 1, 2)\n    list.partitionThreeWay(2)\n    assertEquals(listOf(1, 2, 3), list)\n\n    list = mutableListOf(3, 2, 1)\n    list.partitionThreeWay(2)\n    assertEquals(listOf(1, 2, 3), list)\n  }\n\n  @Test\n  fun fuzzyThreeWay() {\n    val list = ThreadLocalRandom.current().ints(100, 1, 4).toList().toMutableList()\n    val countOne = list.count { it == 1 }\n    val countTwo = list.count { it == 2 }\n    val countThree = list.count { it == 3 }\n    assertEquals(100, countOne + countTwo + countThree)\n\n    list.partitionThreeWay(2)\n\n    for (i in 0..&lt;countOne)\n      assertEquals(1, list[i])\n    for (i in countOne..&lt;countOne+countTwo)\n      assertEquals(2, list[i])\n    for (i in countOne+countTwo..&lt;countOne+countTwo+countThree)\n      assertEquals(3, list[i])\n  }\n}\n</code></pre> <p> = p&lt; p&gt; p0lc</p> <p>All elements \\(\\lt\\) <code>pivot</code> are in range \\([0, l)\\), elements \\(=\\) <code>pivot</code> are in range \\([l, c)\\),  and all elements \\(\\gt\\) <code>pivot</code> in \\([c, \\text{length})\\).</p>"},{"location":"search/binary/","title":"Binary Search","text":""},{"location":"search/binary/#tldr","title":"tl;dr","text":"<p>Finds an element in a sorted list. If not found, returns the place to insert said element.</p> pseudocodekotlintests <pre><code>search(array, needle):\n  l = 0, r = array.length - 1\n  while l &lt;= r:\n    m = (l + r) / 2\n\n    switch(needle):\n      &lt; array[m] =&gt; r = m - 1\n      = array[m] =&gt; return (m, found)\n      &gt; array[m] =&gt; l = m + 1\n\n  return (l, not found)\n</code></pre> <pre><code>fun &lt;T: Comparable&lt;T&gt;&gt; List&lt;T&gt;.binarySearch(needle: T): Pair&lt;Int, Boolean&gt; {\n  var l = 0; var r = size - 1;\n  while (l &lt;= r) {\n    val m = l + (r - l) / 2 // avoids overflow of (l+r)/2\n    when(needle.compareTo(this[m])) {\n      -1 -&gt; r = m - 1\n       0 -&gt; return Pair(m, true)\n       1 -&gt; l = m + 1\n    }\n  }\n  return Pair(l, false) // l and r have swapped boundary\n}\n</code></pre> <pre><code>package search\n\nimport org.junit.jupiter.api.Assertions.*\nimport org.junit.jupiter.api.Test\nimport java.util.concurrent.ThreadLocalRandom\nimport java.util.stream.IntStream\nimport kotlin.streams.toList\n\nclass BinarySearchTest {\n\n  @Test\n  fun searchEmpty() {\n    assertEquals(Pair(0, false), listOf&lt;Int&gt;().binarySearch(10))\n  }\n\n  @Test\n  fun searchOne() {\n    assertEquals(Pair(0, false), listOf(10).binarySearch(9))\n    assertEquals(Pair(0, true), listOf(10).binarySearch(10))\n    assertEquals(Pair(1, false), listOf(10).binarySearch(11))\n  }\n\n  @Test\n  fun searchTwo() {\n    assertEquals(Pair(0, false), listOf(8, 10).binarySearch(7))\n    assertEquals(Pair(0, true), listOf(8, 10).binarySearch(8))\n    assertEquals(Pair(1, false), listOf(8, 10).binarySearch(9))\n    assertEquals(Pair(1, true), listOf(8, 10).binarySearch(10))\n    assertEquals(Pair(2, false), listOf(8, 10).binarySearch(11))\n  }\n\n  @Test\n  fun fuzzySearchExistingKey() {\n    val list = ThreadLocalRandom.current().ints(100, 0, 80).sorted().toList().toMutableList()\n\n    val existingNeedle = list[ThreadLocalRandom.current().nextInt(100)]\n    val got = list.binarySearch(existingNeedle)\n\n    assertTrue(got.second)\n    assertEquals(existingNeedle, list[got.first])\n  }\n\n  @Test\n  fun fuzzySearchBeyondStart() {\n    val list = ThreadLocalRandom.current().ints(100, 0, 80).sorted().toList().toMutableList()\n\n    val nonExistentNeedle = list[0] - 1\n    val got = list.binarySearch(nonExistentNeedle)\n\n    assertFalse(got.second)\n    assertEquals(0, got.first)\n  }\n\n  @Test\n  fun fuzzySearchBeyondEnd() {\n    val list = ThreadLocalRandom.current().ints(100, 0, 80).sorted().toList().toMutableList()\n\n    val nonExistentNeedle = list[list.size - 1] + 1\n    val got = list.binarySearch(nonExistentNeedle)\n\n    assertFalse(got.second)\n    assertEquals(list.size, got.first)\n  }\n\n  @Test\n  fun fuzzySearchMissingInRange() {\n    val list = ThreadLocalRandom.current().ints(100, 0, 80).sorted().toList().toMutableList()\n\n    val nonExistentNeedles = IntStream.range(0, 80).filter{ list.indexOf(it) &lt; 0 }.filter{ it &gt; list.first() &amp;&amp; it &lt; list.last() }.toList()\n    for (n in nonExistentNeedles) {\n      val got = list.binarySearch(n)\n\n      assertFalse(got.second)\n      assertTrue(list[got.first - 1] &lt; n)\n      assertTrue(n &lt; list[got.first])\n    }\n  }\n}\n</code></pre>"},{"location":"search/binary/#examples","title":"Examples","text":"<pre><code> 0  1  2  3  4  : Index\n[0, 2, 4, 6, 8] : Elements\n\ninsert -1 at 0\nfound   0 at 0\ninsert  1 at 1\nfound   2 at 1\ninsert  3 at 2\nfound   4 at 2\ninsert  5 at 3\nfound   6 at 3\ninsert  7 at 4\nfound   8 at 4\ninsert  9 at 5\ninsert 10 at 5\n</code></pre>"},{"location":"sort/mergesort/","title":"Mergesort","text":""},{"location":"sort/mergesort/#tldr","title":"tl;dr","text":"<p>Keep spliting the input in half and sort the halves recursively.</p> pseudocodekotlintests <pre><code>mergesort(array):\n  if array.length \u2264 2:\n    sort it normally\n    return\n\n  left  = array[0, length / 2)\n  right = array[length / 2, array.length)\n  mergesort(left)\n  mergesort(right)\n\n  array = merge(left, right) \n</code></pre> <pre><code>fun &lt;T: Comparable&lt;T&gt;&gt; MutableList&lt;T&gt;.mergesort(start: Int = 0, end: Int = size) {\n  val length = end - start\n  if (length &lt; 2) return\n  if (length == 2) {\n    if (this[start] &gt; this[start + 1]) swap(start, start+1)\n    return\n  }\n\n  val left = this // proxy for [start, start + length / 2)\n  val right = subList(start + length / 2, end).toMutableList()\n  left.mergesort(start, start + length / 2)\n  right.mergesort()\n\n  var l = start + length / 2 - 1; var r = right.size - 1; var c = end - 1\n  while (l &gt;= start &amp;&amp; r &gt;= 0) {\n    if (left[l] &gt; right[r]) this[c--] = left[l--]\n    else this[c--] = right[r--]\n  }\n  while (r &gt;= 0) this[c--] = right[r--]\n}\n\nprivate fun &lt;T&gt; MutableList&lt;T&gt;.swap(i: Int, j: Int) {\n  val t = this[i]\n  this[i] = this[j]\n  this[j] = t\n}\n</code></pre> <pre><code>import org.junit.jupiter.api.Assertions.*\nimport org.junit.jupiter.api.Test\nimport java.util.concurrent.ThreadLocalRandom\nimport kotlin.math.log2\nimport kotlin.streams.toList\n\nclass MergeSortTest {\n\n  @Test\n  fun empty() {\n    val list = mutableListOf&lt;Int&gt;()\n    list.mergesort()\n    assertEquals(listOf&lt;Int&gt;(), list)\n  }\n\n  @Test\n  fun single() {\n    val list = mutableListOf(6)\n    list.mergesort()\n    assertEquals(listOf(6), list)\n  }\n\n  @Test\n  fun double() {\n    var list = mutableListOf(2, 6)\n    list.mergesort()\n    assertEquals(listOf(2, 6), list)\n\n    list = mutableListOf(6, 2)\n    list.mergesort()\n    assertEquals(listOf(2, 6), list)\n  }\n\n  @Test\n  fun triple() {\n    var list = mutableListOf(2, 6, 10)\n    list.mergesort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(2, 10, 6)\n    list.mergesort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(6, 2, 10)\n    list.mergesort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(6, 10, 2)\n    list.mergesort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(10, 2, 6)\n    list.mergesort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(10, 6, 2)\n    list.mergesort()\n    assertEquals(listOf(2, 6, 10), list)\n  }\n\n  @Test\n  fun fuzzy() {\n    val list = ThreadLocalRandom.current().ints(100, 0, 80).toList().toMutableList()\n    val copy = list.toMutableList()\n    assertEquals(list, copy)\n    copy.sort()\n    assertNotEquals(list, copy)\n\n    list.quickSort()\n\n    assertEquals(list, copy)\n  }\n}\n</code></pre> <p>Half the struggle in mergesort is getting the boundaries right. A few points to highlight here:</p> <ol> <li>Splitting and merging should be done with \\([start, end)\\) as range and not \\([0, size)\\).</li> <li>We only move the right half to auxiliary array. So when we recursively sort the left half, make sure to use the right range: \\([start, start + \\frac{end - start}{2})\\). Otherwise, it'll get stuck in an infinite loop, or worse give a non-sense sorting. </li> </ol>"},{"location":"sort/mergesort/#explanation","title":"Explanation","text":"<p> split array in two halvescopy the second half to an auxiliary arraysort the left half of original arraysort the auxiliary array1. Divide2. Conquer3. Combineleft-cursorright-cursorfill back in original</p> <p>First off, we don't need two auxiliary arrays in the divide step. Just one half is enough.Secondly, merge happens over the buffer area. In this case, it's the second half of the original array which is safely copied over in auxiliary array.Were we to copy left-to-right, elements from auxiliary array can overwrite left subarray's content before they can be merged correctly.</p>"},{"location":"sort/mergesort/#benchmark","title":"Benchmark","text":"benchmark code<pre><code>@Test\nfun benchmark() {\n  println(\"['input size','20n log(n)','library sort','homemade sort'],\")\n  for (n in listOf(100_000, 200_000, 300_000, 400_000, 500_000, 600_000, 700_000, 800_000, 900_000, 1_000_000)) {\n    val list = ThreadLocalRandom.current().ints(n.toLong(), 0, n / 2).toList().toMutableList()\n    val copy = list.toList().toMutableList()\n\n    var start = System.nanoTime()\n    list.mergesort()\n    val durationHomemade = System.nanoTime() - start\n\n    start = System.nanoTime()\n    copy.sort()\n    val durationLib = System.nanoTime() - start\n\n    val expected = 20 * n * log2(n.toFloat())\n\n    println(\"[$n,$expected,$durationLib,$durationHomemade],\")\n  }\n}\n</code></pre>"},{"location":"sort/quicksort/","title":"Quicksort","text":""},{"location":"sort/quicksort/#tldr","title":"tl;dr","text":"<p>We start with the partitioning as discussed previously and repeatedly partition the input in smaller and smaller chunks, implicitly sorting them.</p> pseudocodekotlintests <pre><code>quicksort(array):\n  if (array.length &lt; 2) return\n\n  pivotIndex = partition(array)\n  quicksort(array[0, pivotIndex))\n  quicksort(array[pivotIndex, array.length))\n</code></pre> <pre><code>fun &lt;T: Comparable&lt;T&gt;&gt; MutableList&lt;T&gt;.quickSort(start: Int = 0, end: Int = size) {\n  // end is excluded, that's why length is not end - start + 1\n  if (end - start &lt; 2) return\n\n  val (ps, pe) = partition(start, end)\n  quickSort(start, ps)\n  quickSort(pe, end)\n}\n\nprivate fun &lt;T: Comparable&lt;T&gt;&gt; MutableList&lt;T&gt;.partition(start: Int, end: Int): Pair&lt;Int, Int&gt; {\n  var l = start; var c = start; var r = end - 1; val pivot = this[start]\n\n  while (c &lt;= r) {\n    when(this[c].compareTo(pivot)) {\n      -1 -&gt; swap(l++, c++)\n       0 -&gt; c++\n       1 -&gt; swap(c, r--)\n    }\n  }\n\n  return Pair(l, c)\n}\n\nprivate fun &lt;T&gt; MutableList&lt;T&gt;.swap(i: Int, j: Int) {\n  val t = this[i]\n  this[i] = this[j]\n  this[j] = t\n}\n</code></pre> <pre><code>import org.junit.jupiter.api.Assertions.*\nimport org.junit.jupiter.api.Test\nimport java.util.concurrent.ThreadLocalRandom\nimport kotlin.streams.toList\n\nclass QuickSortTest {\n\n  @Test\n  fun empty() {\n    val list = mutableListOf&lt;Int&gt;()\n    list.quickSort()\n    assertEquals(listOf&lt;Int&gt;(), list)\n  }\n\n  @Test\n  fun single() {\n    val list = mutableListOf(6)\n    list.quickSort()\n    assertEquals(listOf(6), list)\n  }\n\n  @Test\n  fun double() {\n    var list = mutableListOf(2, 6)\n    list.quickSort()\n    assertEquals(listOf(2, 6), list)\n\n    list = mutableListOf(6, 2)\n    list.quickSort()\n    assertEquals(listOf(2, 6), list)\n  }\n\n  @Test\n  fun triple() {\n    var list = mutableListOf(2, 6, 10)\n    list.quickSort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(2, 10, 6)\n    list.quickSort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(6, 2, 10)\n    list.quickSort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(6, 10, 2)\n    list.quickSort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(10, 2, 6)\n    list.quickSort()\n    assertEquals(listOf(2, 6, 10), list)\n\n    list = mutableListOf(10, 6, 2)\n    list.quickSort()\n    assertEquals(listOf(2, 6, 10), list)\n  }\n\n  @Test\n  fun fuzzy() {\n    val list = ThreadLocalRandom.current().ints(100, 0, 80).toList().toMutableList()\n    val copy = list.toMutableList()\n    assertEquals(list, copy)\n    copy.sort()\n    assertNotEquals(list, copy)\n\n    list.quickSort()\n\n    assertEquals(list, copy)\n  }\n\n}\n</code></pre>"},{"location":"sort/quicksort/#benchmark","title":"Benchmark","text":"benchmark code<pre><code>@Test\nfun benchmark() {\n  println(\"['input size','20n log(n)','library sort','homemade sort'],\")\n  for (n in listOf(100_000, 200_000, 300_000, 400_000, 500_000, 600_000, 700_000, 800_000, 900_000, 1_000_000)) {\n    val list = ThreadLocalRandom.current().ints(n.toLong(), 0, n / 2).toList().toMutableList()\n    val copy = list.toList().toMutableList()\n\n    var start = System.nanoTime()\n    list.quickSort()\n    val durationHomemade = System.nanoTime() - start\n\n    start = System.nanoTime()\n    copy.sort()\n    val durationLib = System.nanoTime() - start\n\n    val expected = 20 * n * log2(n.toFloat())\n\n    println(\"[$n,$expected,$durationLib,$durationHomemade],\")\n  }\n}\n</code></pre>"}]}